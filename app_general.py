import streamlit as st
import pandas as pd
import numpy as np
import re
import io
import json
from datetime import datetime
from collections import defaultdict
import tempfile
import os

# è®¾ç½®é¡µé¢é…ç½®
st.set_page_config(
    page_title="é”€å”®çº¿ç´¢åˆå¹¶å·¥å…·",
    page_icon="ğŸ“Š",
    layout="wide",
    initial_sidebar_state="expanded"
)

# è®¾ç½®åº”ç”¨æ ‡é¢˜å’Œè¯´æ˜
st.title("ğŸ“Š é”€å”®çº¿ç´¢åˆå¹¶å·¥å…· - Webç‰ˆ")
st.markdown("---")

# åˆå§‹åŒ– session state
if 'df_merged' not in st.session_state:
    st.session_state.df_merged = None
if 'processing_log' not in st.session_state:
    st.session_state.processing_log = []
if 'fixed_autohome_df' not in st.session_state:
    st.session_state.fixed_autohome_df = None

# åˆå§‹åŒ–é”€å”®äººå‘˜åå•
if 'consultant_settings' not in st.session_state:
    st.session_state.consultant_settings = [
        {"å§“å": "é™ˆå©·", "å•ä½": "ä¸Šæµ·å®‰å‰åæµæ±½è½¦æœåŠ¡æœ‰é™å…¬å¸", "æ˜¯å¦å¯ç”¨": True},
        {"å§“å": "å¼ ç†å¹³", "å•ä½": "ä¸Šæµ·å®‰å‰åæµæ±½è½¦æœåŠ¡æœ‰é™å…¬å¸", "æ˜¯å¦å¯ç”¨": True},
        {"å§“å": "é‚µæŒ¯è‰º", "å•ä½": "ä¸Šæµ·å®‰å‰åæµæ±½è½¦æœåŠ¡æœ‰é™å…¬å¸", "æ˜¯å¦å¯ç”¨": True},
        {"å§“å": "è€¿ä½¶", "å•ä½": "ä¸Šæµ·å®‰å‰åæµæ±½è½¦æœåŠ¡æœ‰é™å…¬å¸", "æ˜¯å¦å¯ç”¨": True},
        {"å§“å": "ç¿ä½³è·ƒ", "å•ä½": "å®‰å‰åæµé”€å”®éƒ¨", "æ˜¯å¦å¯ç”¨": False},
        {"å§“å": "é™ˆæ°", "å•ä½": "å®‰å‰åæµé”€å”®éƒ¨", "æ˜¯å¦å¯ç”¨": False}
    ]

# åˆå§‹åŒ–æ˜ å°„è§„åˆ™
if 'car_series_mapping' not in st.session_state:
    st.session_state.car_series_mapping = [
        {"åŸå§‹æ¨¡å¼": r".*GL8.*é™†å°Š.*", "ç›®æ ‡è½¦ç³»": "GL8 è±ªåå•†åŠ¡è½¦", "æ˜¯å¦å¯ç”¨": True},
        {"åŸå§‹æ¨¡å¼": r".*GL8.*é™†ä¸Šå…¬åŠ¡èˆ±.*", "ç›®æ ‡è½¦ç³»": "GL8 é™†ä¸Šå…¬åŠ¡èˆ±", "æ˜¯å¦å¯ç”¨": True},
        {"åŸå§‹æ¨¡å¼": r".*GL8.*é™†å°š.*", "ç›®æ ‡è½¦ç³»": "GL8é™†å°š", "æ˜¯å¦å¯ç”¨": True},
        {"åŸå§‹æ¨¡å¼": r".*GL8.*Avenir.*", "ç›®æ ‡è½¦ç³»": "GL8 Avenir", "æ˜¯å¦å¯ç”¨": True},
        {"åŸå§‹æ¨¡å¼": r".*GL8.*è±ªåå•†åŠ¡è½¦.*", "ç›®æ ‡è½¦ç³»": "GL8 è±ªåå•†åŠ¡è½¦", "æ˜¯å¦å¯ç”¨": True},
        {"åŸå§‹æ¨¡å¼": r".*å›è¶Š.*", "ç›®æ ‡è½¦ç³»": "å…¨æ–°ä¸€ä»£å›è¶Š", "æ˜¯å¦å¯ç”¨": True},
        {"åŸå§‹æ¨¡å¼": r".*å›å¨.*", "ç›®æ ‡è½¦ç³»": "å…¨æ–°ä¸€ä»£å›å¨", "æ˜¯å¦å¯ç”¨": True},
        {"åŸå§‹æ¨¡å¼": r".*æ–°å›å¨.*", "ç›®æ ‡è½¦ç³»": "å…¨æ–°ä¸€ä»£å›å¨", "æ˜¯å¦å¯ç”¨": True},
        {"åŸå§‹æ¨¡å¼": r".*æ˜‚ç§‘å¨Plus.*", "ç›®æ ‡è½¦ç³»": "æ˜‚ç§‘å¨PLUS", "æ˜¯å¦å¯ç”¨": True},
        {"åŸå§‹æ¨¡å¼": r".*æ˜‚ç§‘å¨PLUS.*", "ç›®æ ‡è½¦ç³»": "æ˜‚ç§‘å¨PLUS", "æ˜¯å¦å¯ç”¨": True},
        {"åŸå§‹æ¨¡å¼": r".*æ˜‚ç§‘å¨S.*", "ç›®æ ‡è½¦ç³»": "æ˜‚ç§‘å¨S", "æ˜¯å¦å¯ç”¨": True},
        {"åŸå§‹æ¨¡å¼": r".*å¨æœ—.*", "ç›®æ ‡è½¦ç³»": "å¨æœ—Pro", "æ˜¯å¦å¯ç”¨": True},
        {"åŸå§‹æ¨¡å¼": r".*å¾®è“6.*", "ç›®æ ‡è½¦ç³»": "VELITE 6", "æ˜¯å¦å¯ç”¨": True},
        {"åŸå§‹æ¨¡å¼": r".*VELITE 6.*", "ç›®æ ‡è½¦ç³»": "VELITE 6", "æ˜¯å¦å¯ç”¨": True},
        {"åŸå§‹æ¨¡å¼": r".*E5.*", "ç›®æ ‡è½¦ç³»": "E 5", "æ˜¯å¦å¯ç”¨": True},
        {"åŸå§‹æ¨¡å¼": r".*E 5.*", "ç›®æ ‡è½¦ç³»": "E 5", "æ˜¯å¦å¯ç”¨": True},
        {"åŸå§‹æ¨¡å¼": r".*ä¸–çºª.*", "ç›®æ ‡è½¦ç³»": "ä¸–çºª", "æ˜¯å¦å¯ç”¨": True},
        {"åŸå§‹æ¨¡å¼": r".*ä¸–å®¶.*", "ç›®æ ‡è½¦ç³»": "è‡³å¢ƒä¸–å®¶", "æ˜¯å¦å¯ç”¨": True},
        {"åŸå§‹æ¨¡å¼": r".*L7.*", "ç›®æ ‡è½¦ç³»": "è‡³å¢ƒ", "æ˜¯å¦å¯ç”¨": True},
        {"åŸå§‹æ¨¡å¼": r".*æ˜‚ç§‘æ——.*", "ç›®æ ‡è½¦ç³»": "æ˜‚ç§‘å¨PLUS", "æ˜¯å¦å¯ç”¨": True},
        {"åŸå§‹æ¨¡å¼": r".*åˆ«å…‹.*", "ç›®æ ‡è½¦ç³»": "æ˜‚ç§‘å¨PLUS", "æ˜¯å¦å¯ç”¨": True}
    ]

if 'source_category_mapping' not in st.session_state:
    st.session_state.source_category_mapping = [
        {"åŸå§‹æ¥æº": "è½¦å•†æ±‡", "ç›®æ ‡åˆ†ç±»": "å‚åª’", "æ˜¯å¦å¯ç”¨": True},
        {"åŸå§‹æ¥æº": "è½¦å•†æ±‡ï¼ˆé›†å®¢å·ï¼‰", "ç›®æ ‡åˆ†ç±»": "å‚åª’", "æ˜¯å¦å¯ç”¨": True},
        {"åŸå§‹æ¥æº": "è½¦å•†æ±‡ï¼ˆIMä¼šè¯ï¼‰", "ç›®æ ‡åˆ†ç±»": "å‚åª’", "æ˜¯å¦å¯ç”¨": True},
        {"åŸå§‹æ¥æº": "è½¦å•†æ±‡ï¼ˆåˆ†æœŸï¼‰", "ç›®æ ‡åˆ†ç±»": "å‚åª’", "æ˜¯å¦å¯ç”¨": True},
        {"åŸå§‹æ¥æº": "è½¦å•†æ±‡ï¼ˆå¹³å°æ´»åŠ¨ï¼‰", "ç›®æ ‡åˆ†ç±»": "å‚åª’", "æ˜¯å¦å¯ç”¨": True},
        {"åŸå§‹æ¥æº": "æ™ºèƒ½äº§å“ï¼ˆæ™ºèƒ½å±•å…ï¼‰", "ç›®æ ‡åˆ†ç±»": "å‚åª’", "æ˜¯å¦å¯ç”¨": True},
        {"åŸå§‹æ¥æº": "æŠ–éŸ³", "ç›®æ ‡åˆ†ç±»": "è‡ªåª’", "æ˜¯å¦å¯ç”¨": True},
        {"åŸå§‹æ¥æº": "æœ¬åœ°é€š-ç»é”€å•†å·", "ç›®æ ‡åˆ†ç±»": "è‡ªåª’", "æ˜¯å¦å¯ç”¨": True},
        {"åŸå§‹æ¥æº": "æœ¬åœ°é€šå¼‚åœ°-ç»é”€å•†å·", "ç›®æ ‡åˆ†ç±»": "è‡ªåª’", "æ˜¯å¦å¯ç”¨": True},
        {"åŸå§‹æ¥æº": "æœ¬åœ°é€š", "ç›®æ ‡åˆ†ç±»": "è‡ªåª’", "æ˜¯å¦å¯ç”¨": True},
         {"åŸå§‹æ¥æº": "æ˜“è½¦ç½‘", "ç›®æ ‡åˆ†ç±»": "å‚åª’", "æ˜¯å¦å¯ç”¨": True},
        {"åŸå§‹æ¥æº": "æ±½è½¦ä¹‹å®¶", "ç›®æ ‡åˆ†ç±»": "å‚åª’", "æ˜¯å¦å¯ç”¨": True},
        {"åŸå§‹æ¥æº": "åˆ«å…‹ç§åŸŸ", "ç›®æ ‡åˆ†ç±»": "ä¸»æœºå‚ä¸‹å‘", "æ˜¯å¦å¯ç”¨": True},
        {"åŸå§‹æ¥æº": "iBuick", "ç›®æ ‡åˆ†ç±»": "ä¸»æœºå‚ä¸‹å‘", "æ˜¯å¦å¯ç”¨": True},
        # æ–°æ·»åŠ çš„è§„åˆ™
        {"åŸå§‹æ¥æº": "æ€»éƒ¨çŸ©é˜µå·", "ç›®æ ‡åˆ†ç±»": "ä¸»æœºå‚ä¸‹å‘", "æ˜¯å¦å¯ç”¨": True},
        {"åŸå§‹æ¥æº": "ç»é”€å•†å¸‚åœºæ´»åŠ¨", "ç›®æ ‡åˆ†ç±»": "æ’é™¤", "æ˜¯å¦å¯ç”¨": True},
        {"åŸå§‹æ¥æº": "é«˜å¾·åœ°å›¾", "ç›®æ ‡åˆ†ç±»": "ä¸»æœºå‚ä¸‹å‘", "æ˜¯å¦å¯ç”¨": True},
        {"åŸå§‹æ¥æº": "çŸ©é˜µå·", "ç›®æ ‡åˆ†ç±»": "ä¸»æœºå‚ä¸‹å‘", "æ˜¯å¦å¯ç”¨": True},
    ]

if 'source_detail_mapping' not in st.session_state:
    st.session_state.source_detail_mapping = [
        {"åŸå§‹æ¥æº": "è½¦å•†æ±‡", "ç›®æ ‡çº¿ç´¢æ¥æº": "æ±½è½¦ä¹‹å®¶", "æ˜¯å¦å¯ç”¨": True},
        {"åŸå§‹æ¥æº": "è½¦å•†æ±‡(é›†å®¢å·)", "ç›®æ ‡çº¿ç´¢æ¥æº": "æ±½è½¦ä¹‹å®¶", "æ˜¯å¦å¯ç”¨": True},
        {"åŸå§‹æ¥æº": "è½¦å•†æ±‡ï¼ˆIMä¼šè¯ï¼‰", "ç›®æ ‡çº¿ç´¢æ¥æº": "æ±½è½¦ä¹‹å®¶", "æ˜¯å¦å¯ç”¨": True},
        {"åŸå§‹æ¥æº": "è½¦å•†æ±‡ï¼ˆåˆ†æœŸï¼‰", "ç›®æ ‡çº¿ç´¢æ¥æº": "æ±½è½¦ä¹‹å®¶", "æ˜¯å¦å¯ç”¨": True},
        {"åŸå§‹æ¥æº": "è½¦å•†æ±‡ï¼ˆå¹³å°æ´»åŠ¨ï¼‰", "ç›®æ ‡çº¿ç´¢æ¥æº": "æ±½è½¦ä¹‹å®¶", "æ˜¯å¦å¯ç”¨": True},
        {"åŸå§‹æ¥æº": "æ™ºèƒ½äº§å“ï¼ˆæ™ºèƒ½å±•å…ï¼‰", "ç›®æ ‡çº¿ç´¢æ¥æº": "æ±½è½¦ä¹‹å®¶", "æ˜¯å¦å¯ç”¨": True},
        {"åŸå§‹æ¥æº": "æŠ–éŸ³", "ç›®æ ‡çº¿ç´¢æ¥æº": "æŠ–éŸ³", "æ˜¯å¦å¯ç”¨": True},
        {"åŸå§‹æ¥æº": "æœ¬åœ°é€š-ç»é”€å•†å·", "ç›®æ ‡çº¿ç´¢æ¥æº": "æŠ–éŸ³", "æ˜¯å¦å¯ç”¨": True},
        {"åŸå§‹æ¥æº": "æœ¬åœ°é€šå¼‚åœ°-ç»é”€å•†å·", "ç›®æ ‡çº¿ç´¢æ¥æº": "æŠ–éŸ³", "æ˜¯å¦å¯ç”¨": True},
        {"åŸå§‹æ¥æº": "æœ¬åœ°", "ç›®æ ‡çº¿ç´¢æ¥æº": "æŠ–éŸ³", "æ˜¯å¦å¯ç”¨": True},
        {"åŸå§‹æ¥æº": "æ˜“è½¦ç½‘", "ç›®æ ‡çº¿ç´¢æ¥æº": "æ˜“è½¦", "æ˜¯å¦å¯ç”¨": True},
        {"åŸå§‹æ¥æº": "æ±½è½¦ä¹‹å®¶", "ç›®æ ‡çº¿ç´¢æ¥æº": "æ±½è½¦ä¹‹å®¶", "æ˜¯å¦å¯ç”¨": True},
        {"åŸå§‹æ¥æº": "åˆ«å…‹ç§åŸŸ", "ç›®æ ‡åˆ†ç±»": "", "æ˜¯å¦å¯ç”¨": True},
        {"åŸå§‹æ¥æº": "iBuick", "ç›®æ ‡çº¿ç´¢æ¥æº": "", "æ˜¯å¦å¯ç”¨": True},
        # æ–°æ·»åŠ çš„è§„åˆ™
        {"åŸå§‹æ¥æº": "æ€»éƒ¨çŸ©é˜µå·", "ç›®æ ‡çº¿ç´¢æ¥æº": "", "æ˜¯å¦å¯ç”¨": True},
        {"åŸå§‹æ¥æº": "ç»é”€å•†å¸‚åœºæ´»åŠ¨", "ç›®æ ‡çº¿ç´¢æ¥æº": "æ’é™¤", "æ˜¯å¦å¯ç”¨": True},  # ç‰¹æ®Šæ ‡è®°ï¼Œç”¨äºæ’é™¤
        {"åŸå§‹æ¥æº": "é«˜å¾·åœ°å›¾", "ç›®æ ‡çº¿ç´¢æ¥æº": "", "æ˜¯å¦å¯ç”¨": True},
        {"åŸå§‹æ¥æº": "çŸ©é˜µå·", "ç›®æ ‡çº¿ç´¢æ¥æº": "", "æ˜¯å¦å¯ç”¨": True},
    ]

def add_log(message):
    """æ·»åŠ å¤„ç†æ—¥å¿—"""
    st.session_state.processing_log.append(f"{datetime.now().strftime('%H:%M:%S')} - {message}")

# ä¾§è¾¹æ é…ç½®
st.sidebar.header("âš™ï¸ é…ç½®é€‰é¡¹")

# 1. æ–‡ä»¶æ ¼å¼ä¿®å¤éƒ¨åˆ†
st.sidebar.subheader("1. æ–‡ä»¶æ ¼å¼ä¿®å¤")
uploaded_file = st.sidebar.file_uploader(
    "ä¸Šä¼ æ±½è½¦ä¹‹å®¶CSVæ–‡ä»¶",
    type=['csv'],
    help="ä¸Šä¼ éœ€è¦ä¿®å¤æ ¼å¼çš„æ±½è½¦ä¹‹å®¶CSVæ–‡ä»¶",
    key="autohome_original"
)

if uploaded_file is not None:
    st.sidebar.success(f"å·²ä¸Šä¼ : {uploaded_file.name}")
    
    # ä¿®å¤CSVæ ¼å¼çš„å‡½æ•°
    def fix_csv_format(file_content):
        """ä¿®å¤CSVæ ¼å¼é—®é¢˜"""
        try:
            # å°è¯•å¤šç§æ–¹å¼è¯»å–
            content = file_content.decode('utf-8-sig')
        except:
            try:
                content = file_content.decode('gbk')
            except:
                content = file_content.decode('utf-8', errors='ignore')
        
        lines = content.splitlines()
        processed_lines = []
        
        for i, line in enumerate(lines):
            line = line.strip()
            
            if i == 0:
                # æ ‡é¢˜è¡Œ
                processed_lines.append(line)
                continue
            
            if not line:  # è·³è¿‡ç©ºè¡Œ
                continue
            
            # å¤„ç†å¼•å·
            if line.startswith('"') and line.endswith('"'):
                line = line[1:-1]
            
            # å¤„ç†è½¬ä¹‰çš„åŒå¼•å·
            line = line.replace('""', 'TEMP_QUOTE')
            
            # åˆ†å‰²å­—æ®µ
            parts = []
            current_part = []
            in_quotes = False
            
            for char in line:
                if char == '"' and not in_quotes:
                    in_quotes = True
                elif char == '"' and in_quotes:
                    in_quotes = False
                elif char == ',' and not in_quotes:
                    parts.append(''.join(current_part))
                    current_part = []
                else:
                    current_part.append(char)
            
            if current_part:
                parts.append(''.join(current_part))
            
            # æ¢å¤è½¬ä¹‰çš„åŒå¼•å·
            processed_parts = []
            for part in parts:
                if part.startswith('"') and part.endswith('"'):
                    part = part[1:-1]
                part = part.replace('TEMP_QUOTE', '"')
                processed_parts.append(part)
            
            processed_lines.append(','.join(processed_parts))
        
        return '\n'.join(processed_lines)
    
    # æ˜¾ç¤ºä¿®å¤é€‰é¡¹
    if st.sidebar.button("ä¿®å¤æ–‡ä»¶æ ¼å¼"):
        try:
            fixed_content = fix_csv_format(uploaded_file.getvalue())
            
            # è¯»å–ä¿®å¤åçš„å†…å®¹åˆ°DataFrame
            try:
                fixed_df = pd.read_csv(io.StringIO(fixed_content))
                st.session_state.fixed_autohome_df = fixed_df
                
                # æä¾›ä¸‹è½½ä¿®å¤åçš„æ–‡ä»¶
                st.sidebar.download_button(
                    label="ä¸‹è½½ä¿®å¤åçš„æ–‡ä»¶",
                    data=fixed_content,
                    file_name=f"fixed_{uploaded_file.name}",
                    mime="text/csv"
                )
                st.sidebar.success(f"æ–‡ä»¶æ ¼å¼ä¿®å¤å®Œæˆï¼å…± {len(fixed_df)} æ¡è®°å½•")
                
                # æ˜¾ç¤ºä¿®å¤åçš„æ•°æ®é¢„è§ˆ
                with st.sidebar.expander("æŸ¥çœ‹ä¿®å¤åçš„æ•°æ®é¢„è§ˆ"):
                    st.dataframe(fixed_df.head(5))
                    
            except Exception as e:
                st.sidebar.error(f"è¯»å–ä¿®å¤åçš„æ–‡ä»¶å¤±è´¥: {str(e)}")
                
        except Exception as e:
            st.sidebar.error(f"ä¿®å¤å¤±è´¥: {str(e)}")

# 2. æ˜ å°„è§„åˆ™ç®¡ç†
with st.sidebar.expander("ğŸ—ºï¸ æ˜ å°„è§„åˆ™ç®¡ç†", expanded=False):
    tab1, tab2, tab3 = st.tabs(["ğŸš— è½¦ç³»æ˜ å°„", "ğŸ“Š æ¥æºåˆ†ç±»", "ğŸ” çº¿ç´¢æ¥æº"])
    
    with tab1:
        st.write("è½¦ç³»åç§°æ˜ å°„è§„åˆ™ï¼š")
        car_mapping_df = pd.DataFrame(st.session_state.car_series_mapping)
        edited_car_df = st.data_editor(
            car_mapping_df,
            column_config={
                "åŸå§‹æ¨¡å¼": st.column_config.TextColumn("åŸå§‹æ¨¡å¼(æ”¯æŒæ­£åˆ™)", width="large", required=True, help="ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼åŒ¹é…åŸå§‹è½¦ç³»åç§°"),
                "ç›®æ ‡è½¦ç³»": st.column_config.TextColumn("ç›®æ ‡è½¦ç³»", width="medium", required=True),
                "æ˜¯å¦å¯ç”¨": st.column_config.CheckboxColumn("æ˜¯å¦å¯ç”¨", default=True)
            },
            num_rows="dynamic",
            key="car_mapping_editor"
        )
        
        col1, col2 = st.columns(2)
        with col1:
            if st.button("ğŸ’¾ ä¿å­˜è½¦ç³»è§„åˆ™", use_container_width=True, key="save_car_mapping"):
                st.session_state.car_series_mapping = edited_car_df.to_dict('records')
                st.success("è½¦ç³»æ˜ å°„è§„åˆ™å·²æ›´æ–°ï¼")
        
        with col2:
            if st.button("ğŸ”„ æ¢å¤é»˜è®¤", use_container_width=True, key="reset_car_mapping"):
                st.session_state.car_series_mapping = [
                    {"åŸå§‹æ¨¡å¼": r".*GL8.*é™†å°Š.*", "ç›®æ ‡è½¦ç³»": "GL8 è±ªåå•†åŠ¡è½¦", "æ˜¯å¦å¯ç”¨": True},
                    {"åŸå§‹æ¨¡å¼": r".*GL8.*é™†ä¸Šå…¬åŠ¡èˆ±.*", "ç›®æ ‡è½¦ç³»": "GL8 é™†ä¸Šå…¬åŠ¡èˆ±", "æ˜¯å¦å¯ç”¨": True},
                    {"åŸå§‹æ¨¡å¼": r".*GL8.*é™†å°š.*", "ç›®æ ‡è½¦ç³»": "GL8é™†å°š", "æ˜¯å¦å¯ç”¨": True},
                    {"åŸå§‹æ¨¡å¼": r".*GL8.*Avenir.*", "ç›®æ ‡è½¦ç³»": "GL8 Avenir", "æ˜¯å¦å¯ç”¨": True},
                    {"åŸå§‹æ¨¡å¼": r".*GL8.*è±ªåå•†åŠ¡è½¦.*", "ç›®æ ‡è½¦ç³»": "GL8 è±ªåå•†åŠ¡è½¦", "æ˜¯å¦å¯ç”¨": True},
                    {"åŸå§‹æ¨¡å¼": r".*å›è¶Š.*", "ç›®æ ‡è½¦ç³»": "å…¨æ–°ä¸€ä»£å›è¶Š", "æ˜¯å¦å¯ç”¨": True},
                    {"åŸå§‹æ¨¡å¼": r".*å›å¨.*", "ç›®æ ‡è½¦ç³»": "å…¨æ–°ä¸€ä»£å›å¨", "æ˜¯å¦å¯ç”¨": True},
                    {"åŸå§‹æ¨¡å¼": r".*æ–°å›å¨.*", "ç›®æ ‡è½¦ç³»": "å…¨æ–°ä¸€ä»£å›å¨", "æ˜¯å¦å¯ç”¨": True},
                    {"åŸå§‹æ¨¡å¼": r".*æ˜‚ç§‘å¨Plus.*", "ç›®æ ‡è½¦ç³»": "æ˜‚ç§‘å¨PLUS", "æ˜¯å¦å¯ç”¨": True},
                    {"åŸå§‹æ¨¡å¼": r".*æ˜‚ç§‘å¨PLUS.*", "ç›®æ ‡è½¦ç³»": "æ˜‚ç§‘å¨PLUS", "æ˜¯å¦å¯ç”¨": True},
                    {"åŸå§‹æ¨¡å¼": r".*æ˜‚ç§‘å¨S.*", "ç›®æ ‡è½¦ç³»": "æ˜‚ç§‘å¨S", "æ˜¯å¦å¯ç”¨": True},
                    {"åŸå§‹æ¨¡å¼": r".*å¨æœ—.*", "ç›®æ ‡è½¦ç³»": "å¨æœ—Pro", "æ˜¯å¦å¯ç”¨": True},
                    {"åŸå§‹æ¨¡å¼": r".*å¾®è“6.*", "ç›®æ ‡è½¦ç³»": "VELITE 6", "æ˜¯å¦å¯ç”¨": True},
                    {"åŸå§‹æ¨¡å¼": r".*VELITE 6.*", "ç›®æ ‡è½¦ç³»": "VELITE 6", "æ˜¯å¦å¯ç”¨": True},
                    {"åŸå§‹æ¨¡å¼": r".*E5.*", "ç›®æ ‡è½¦ç³»": "E 5", "æ˜¯å¦å¯ç”¨": True},
                    {"åŸå§‹æ¨¡å¼": r".*E 5.*", "ç›®æ ‡è½¦ç³»": "E 5", "æ˜¯å¦å¯ç”¨": True},
                    {"åŸå§‹æ¨¡å¼": r".*ä¸–çºª.*", "ç›®æ ‡è½¦ç³»": "ä¸–çºª", "æ˜¯å¦å¯ç”¨": True},
                    {"åŸå§‹æ¨¡å¼": r".*ä¸–å®¶.*", "ç›®æ ‡è½¦ç³»": "è‡³å¢ƒä¸–å®¶", "æ˜¯å¦å¯ç”¨": True},
                    {"åŸå§‹æ¨¡å¼": r".*L7.*", "ç›®æ ‡è½¦ç³»": "è‡³å¢ƒ", "æ˜¯å¦å¯ç”¨": True},
                    {"åŸå§‹æ¨¡å¼": r".*æ˜‚ç§‘æ——.*", "ç›®æ ‡è½¦ç³»": "æ˜‚ç§‘å¨PLUS", "æ˜¯å¦å¯ç”¨": True},
                    {"åŸå§‹æ¨¡å¼": r".*åˆ«å…‹.*", "ç›®æ ‡è½¦ç³»": "æ˜‚ç§‘å¨PLUS", "æ˜¯å¦å¯ç”¨": True}
                ]
                st.success("å·²æ¢å¤é»˜è®¤è½¦ç³»æ˜ å°„è§„åˆ™ï¼")
    
    with tab2:
        st.write("æ¥æºåˆ†ç±»æ˜ å°„è§„åˆ™ï¼š")
        st.markdown("""
        **ç‰¹æ®Šè§„åˆ™è¯´æ˜ï¼š**
        è®¾ç½®ç›®æ ‡åˆ†ç±»ä¸º`æ’é™¤`æ—¶ï¼Œç³»ç»Ÿä¼šè‡ªåŠ¨å‰”é™¤è¯¥æ¥æºçš„çº¿ç´¢
        """)
        category_mapping_df = pd.DataFrame(st.session_state.source_category_mapping)
        edited_category_df = st.data_editor(
            category_mapping_df,
            column_config={
                "åŸå§‹æ¥æº": st.column_config.TextColumn("åŸå§‹æ¥æº", width="large", required=True),
                "ç›®æ ‡åˆ†ç±»": st.column_config.TextColumn("ç›®æ ‡åˆ†ç±»", width="medium", required=True, help="è®¾ç½®ä¸º'æ’é™¤'ä¼šå‰”é™¤è¯¥æ¥æºçš„çº¿ç´¢"),
                "æ˜¯å¦å¯ç”¨": st.column_config.CheckboxColumn("æ˜¯å¦å¯ç”¨", default=True)
            },
            num_rows="dynamic",
            key="category_mapping_editor"
        )
        
        col1, col2 = st.columns(2)
        with col1:
            if st.button("ğŸ’¾ ä¿å­˜åˆ†ç±»è§„åˆ™", use_container_width=True, key="save_category_mapping"):
                st.session_state.source_category_mapping = edited_category_df.to_dict('records')
                st.success("æ¥æºåˆ†ç±»æ˜ å°„è§„åˆ™å·²æ›´æ–°ï¼")
        
        with col2:
            if st.button("ğŸ”„ æ¢å¤é»˜è®¤", use_container_width=True, key="reset_category_mapping"):
                st.session_state.source_category_mapping = [
                    {"åŸå§‹æ¥æº": "è½¦å•†æ±‡", "ç›®æ ‡åˆ†ç±»": "å‚åª’", "æ˜¯å¦å¯ç”¨": True},
                    {"åŸå§‹æ¥æº": "è½¦å•†æ±‡ï¼ˆé›†å®¢å·ï¼‰", "ç›®æ ‡åˆ†ç±»": "å‚åª’", "æ˜¯å¦å¯ç”¨": True},
                    {"åŸå§‹æ¥æº": "è½¦å•†æ±‡ï¼ˆIMä¼šè¯ï¼‰", "ç›®æ ‡åˆ†ç±»": "å‚åª’", "æ˜¯å¦å¯ç”¨": True},
                    {"åŸå§‹æ¥æº": "è½¦å•†æ±‡ï¼ˆåˆ†æœŸï¼‰", "ç›®æ ‡åˆ†ç±»": "å‚åª’", "æ˜¯å¦å¯ç”¨": True},
                    {"åŸå§‹æ¥æº": "è½¦å•†æ±‡ï¼ˆå¹³å°æ´»åŠ¨ï¼‰", "ç›®æ ‡åˆ†ç±»": "å‚åª’", "æ˜¯å¦å¯ç”¨": True},
                    {"åŸå§‹æ¥æº": "æ™ºèƒ½äº§å“ï¼ˆæ™ºèƒ½å±•å…ï¼‰", "ç›®æ ‡åˆ†ç±»": "å‚åª’", "æ˜¯å¦å¯ç”¨": True},
                    {"åŸå§‹æ¥æº": "æŠ–éŸ³", "ç›®æ ‡åˆ†ç±»": "è‡ªåª’", "æ˜¯å¦å¯ç”¨": True},
                    {"åŸå§‹æ¥æº": "æœ¬åœ°é€š-ç»é”€å•†å·", "ç›®æ ‡åˆ†ç±»": "è‡ªåª’", "æ˜¯å¦å¯ç”¨": True},
                    {"åŸå§‹æ¥æº": "æœ¬åœ°é€šå¼‚åœ°-ç»é”€å•†å·", "ç›®æ ‡åˆ†ç±»": "è‡ªåª’", "æ˜¯å¦å¯ç”¨": True},
                    {"åŸå§‹æ¥æº": "æœ¬åœ°é€š", "ç›®æ ‡åˆ†ç±»": "è‡ªåª’", "æ˜¯å¦å¯ç”¨": True},
                    {"åŸå§‹æ¥æº": "æ˜“è½¦ç½‘", "ç›®æ ‡åˆ†ç±»": "å‚åª’", "æ˜¯å¦å¯ç”¨": True},
                    {"åŸå§‹æ¥æº": "æ±½è½¦ä¹‹å®¶", "ç›®æ ‡åˆ†ç±»": "å‚åª’", "æ˜¯å¦å¯ç”¨": True},
                    {"åŸå§‹æ¥æº": "åˆ«å…‹ç§åŸŸ", "ç›®æ ‡åˆ†ç±»": "ä¸»æœºå‚ä¸‹å‘", "æ˜¯å¦å¯ç”¨": True},
                    {"åŸå§‹æ¥æº": "iBuick", "ç›®æ ‡åˆ†ç±»": "ä¸»æœºå‚ä¸‹å‘", "æ˜¯å¦å¯ç”¨": True},
                    # æ–°æ·»åŠ çš„è§„åˆ™
                    {"åŸå§‹æ¥æº": "æ€»éƒ¨çŸ©é˜µå·", "ç›®æ ‡åˆ†ç±»": "ä¸»æœºå‚ä¸‹å‘", "æ˜¯å¦å¯ç”¨": True},
                    {"åŸå§‹æ¥æº": "ç»é”€å•†å¸‚åœºæ´»åŠ¨", "ç›®æ ‡åˆ†ç±»": "æ’é™¤", "æ˜¯å¦å¯ç”¨": True},
                    {"åŸå§‹æ¥æº": "é«˜å¾·åœ°å›¾", "ç›®æ ‡åˆ†ç±»": "ä¸»æœºå‚ä¸‹å‘", "æ˜¯å¦å¯ç”¨": True},
                    {"åŸå§‹æ¥æº": "çŸ©é˜µå·", "ç›®æ ‡åˆ†ç±»": "ä¸»æœºå‚ä¸‹å‘", "æ˜¯å¦å¯ç”¨": True},
                ]
                st.success("å·²æ¢å¤é»˜è®¤æ¥æºåˆ†ç±»æ˜ å°„è§„åˆ™ï¼")
    
    with tab3:
        st.write("çº¿ç´¢æ¥æºæ˜ å°„è§„åˆ™ï¼š")
        st.markdown("""
        **ç‰¹æ®Šè§„åˆ™è¯´æ˜ï¼š**
        - è®¾ç½®ç›®æ ‡çº¿ç´¢æ¥æºä¸º`æ’é™¤`æ—¶ï¼Œç³»ç»Ÿä¼šè‡ªåŠ¨å‰”é™¤è¯¥æ¥æºçš„çº¿ç´¢
        """)
        detail_mapping_df = pd.DataFrame(st.session_state.source_detail_mapping)
        edited_detail_df = st.data_editor(
            detail_mapping_df,
            column_config={
                "åŸå§‹æ¥æº": st.column_config.TextColumn("åŸå§‹æ¥æº", width="large", required=True),
                "ç›®æ ‡çº¿ç´¢æ¥æº": st.column_config.TextColumn("ç›®æ ‡çº¿ç´¢æ¥æº", width="medium", required=True, help="è®¾ç½®ä¸º'æ’é™¤'ä¼šå‰”é™¤è¯¥æ¥æºçš„çº¿ç´¢"),
                "æ˜¯å¦å¯ç”¨": st.column_config.CheckboxColumn("æ˜¯å¦å¯ç”¨", default=True)
            },
            num_rows="dynamic",
            key="detail_mapping_editor"
        )
        
        col1, col2 = st.columns(2)
        with col1:
            if st.button("ğŸ’¾ ä¿å­˜çº¿ç´¢è§„åˆ™", use_container_width=True, key="save_detail_mapping"):
                st.session_state.source_detail_mapping = edited_detail_df.to_dict('records')
                st.success("çº¿ç´¢æ¥æºæ˜ å°„è§„åˆ™å·²æ›´æ–°ï¼")
        
        with col2:
            if st.button("ğŸ”„ æ¢å¤é»˜è®¤", use_container_width=True, key="reset_detail_mapping"):
                st.session_state.source_detail_mapping = [
                    {"åŸå§‹æ¥æº": "è½¦å•†æ±‡", "ç›®æ ‡çº¿ç´¢æ¥æº": "æ±½è½¦ä¹‹å®¶", "æ˜¯å¦å¯ç”¨": True},
                    {"åŸå§‹æ¥æº": "è½¦å•†æ±‡(é›†å®¢å·)", "ç›®æ ‡çº¿ç´¢æ¥æº": "æ±½è½¦ä¹‹å®¶", "æ˜¯å¦å¯ç”¨": True},
                    {"åŸå§‹æ¥æº": "è½¦å•†æ±‡ï¼ˆIMä¼šè¯ï¼‰", "ç›®æ ‡çº¿ç´¢æ¥æº": "æ±½è½¦ä¹‹å®¶", "æ˜¯å¦å¯ç”¨": True},
                    {"åŸå§‹æ¥æº": "è½¦å•†æ±‡ï¼ˆåˆ†æœŸï¼‰", "ç›®æ ‡çº¿ç´¢æ¥æº": "æ±½è½¦ä¹‹å®¶", "æ˜¯å¦å¯ç”¨": True},
                    {"åŸå§‹æ¥æº": "è½¦å•†æ±‡ï¼ˆå¹³å°æ´»åŠ¨ï¼‰", "ç›®æ ‡çº¿ç´¢æ¥æº": "æ±½è½¦ä¹‹å®¶", "æ˜¯å¦å¯ç”¨": True},
                    {"åŸå§‹æ¥æº": "æ™ºèƒ½äº§å“ï¼ˆæ™ºèƒ½å±•å…ï¼‰", "ç›®æ ‡çº¿ç´¢æ¥æº": "æ±½è½¦ä¹‹å®¶", "æ˜¯å¦å¯ç”¨": True},
                    {"åŸå§‹æ¥æº": "æŠ–éŸ³", "ç›®æ ‡çº¿ç´¢æ¥æº": "æŠ–éŸ³", "æ˜¯å¦å¯ç”¨": True},
                    {"åŸå§‹æ¥æº": "æœ¬åœ°é€š-ç»é”€å•†å·", "ç›®æ ‡çº¿ç´¢æ¥æº": "æŠ–éŸ³", "æ˜¯å¦å¯ç”¨": True},
                    {"åŸå§‹æ¥æº": "æœ¬åœ°é€šå¼‚åœ°-ç»é”€å•†å·", "ç›®æ ‡çº¿ç´¢æ¥æº": "æŠ–éŸ³", "æ˜¯å¦å¯ç”¨": True},
                    {"åŸå§‹æ¥æº": "æœ¬åœ°", "ç›®æ ‡çº¿ç´¢æ¥æº": "æŠ–éŸ³", "æ˜¯å¦å¯ç”¨": True},
                    {"åŸå§‹æ¥æº": "æ˜“è½¦ç½‘", "ç›®æ ‡çº¿ç´¢æ¥æº": "æ˜“è½¦", "æ˜¯å¦å¯ç”¨": True},
                    {"åŸå§‹æ¥æº": "æ±½è½¦ä¹‹å®¶", "ç›®æ ‡çº¿ç´¢æ¥æº": "æ±½è½¦ä¹‹å®¶", "æ˜¯å¦å¯ç”¨": True},
                    {"åŸå§‹æ¥æº": "åˆ«å…‹ç§åŸŸ", "ç›®æ ‡åˆ†ç±»": "", "æ˜¯å¦å¯ç”¨": True},
                    {"åŸå§‹æ¥æº": "iBuick", "ç›®æ ‡çº¿ç´¢æ¥æº": "", "æ˜¯å¦å¯ç”¨": True},
                    # æ–°æ·»åŠ çš„è§„åˆ™
                    {"åŸå§‹æ¥æº": "æ€»éƒ¨çŸ©é˜µå·", "ç›®æ ‡çº¿ç´¢æ¥æº": "", "æ˜¯å¦å¯ç”¨": True},
                    {"åŸå§‹æ¥æº": "ç»é”€å•†å¸‚åœºæ´»åŠ¨", "ç›®æ ‡çº¿ç´¢æ¥æº": "æ’é™¤", "æ˜¯å¦å¯ç”¨": True},
                    {"åŸå§‹æ¥æº": "é«˜å¾·åœ°å›¾", "ç›®æ ‡çº¿ç´¢æ¥æº": "", "æ˜¯å¦å¯ç”¨": True},
                    {"åŸå§‹æ¥æº": "çŸ©é˜µå·", "ç›®æ ‡çº¿ç´¢æ¥æº": "", "æ˜¯å¦å¯ç”¨": True},
                ]
                st.success("å·²æ¢å¤é»˜è®¤çº¿ç´¢æ¥æºæ˜ å°„è§„åˆ™ï¼")
    
    # å¯¼å…¥/å¯¼å‡ºåŠŸèƒ½
    st.write("---")
    st.write("å¯¼å…¥/å¯¼å‡ºæ˜ å°„è§„åˆ™ï¼š")
    
    col3, col4 = st.columns(2)
    with col3:
        # å¯¼å‡ºæ‰€æœ‰æ˜ å°„è§„åˆ™
        all_mappings = {
            "car_series_mapping": st.session_state.car_series_mapping,
            "source_category_mapping": st.session_state.source_category_mapping,
            "source_detail_mapping": st.session_state.source_detail_mapping
        }
        settings_json = json.dumps(all_mappings, ensure_ascii=False, indent=2)
        st.download_button(
            label="ğŸ“¥ å¯¼å‡ºæ‰€æœ‰è§„åˆ™",
            data=settings_json,
            file_name="æ˜ å°„è§„åˆ™é…ç½®.json",
            mime="application/json",
            use_container_width=True
        )
    
    with col4:
        # å¯¼å…¥æ˜ å°„è§„åˆ™
        uploaded_mappings = st.file_uploader("å¯¼å…¥è§„åˆ™æ–‡ä»¶", type=['json'], key="mappings_upload", label_visibility="collapsed")
        if uploaded_mappings:
            try:
                new_mappings = json.load(uploaded_mappings)
                if "car_series_mapping" in new_mappings:
                    st.session_state.car_series_mapping = new_mappings["car_series_mapping"]
                if "source_category_mapping" in new_mappings:
                    st.session_state.source_category_mapping = new_mappings["source_category_mapping"]
                if "source_detail_mapping" in new_mappings:
                    st.session_state.source_detail_mapping = new_mappings["source_detail_mapping"]
                st.success("æ˜ å°„è§„åˆ™å¯¼å…¥æˆåŠŸï¼")
            except Exception as e:
                st.error(f"å¯¼å…¥å¤±è´¥: {str(e)}")

# 3. é”€å”®äººå‘˜ç®¡ç†
with st.sidebar.expander("ğŸ‘¥ é”€å”®äººå‘˜ç®¡ç†", expanded=False):
    st.write("ä¿®æ”¹é”€å”®äººå‘˜åå•å’Œå¯¹åº”å•ä½ï¼š")
    
    # æ˜¾ç¤ºå½“å‰é”€å”®äººå‘˜åå•
    st.write("å½“å‰é”€å”®äººå‘˜åå•ï¼š")
    consultant_df = pd.DataFrame(st.session_state.consultant_settings)
    edited_df = st.data_editor(
        consultant_df,
        column_config={
            "å§“å": st.column_config.TextColumn("å§“å", width="medium", required=True),
            "å•ä½": st.column_config.TextColumn("å•ä½", width="large", required=True),
            "æ˜¯å¦å¯ç”¨": st.column_config.CheckboxColumn("æ˜¯å¦å¯ç”¨", default=True)
        },
        num_rows="dynamic",
        key="consultant_editor"
    )
    
    # ä¿å­˜ä¿®æ”¹æŒ‰é’®
    col1, col2 = st.columns(2)
    with col1:
        if st.button("ğŸ’¾ ä¿å­˜ä¿®æ”¹", use_container_width=True, key="save_consultants"):
            # æ›´æ–°é”€å”®äººå‘˜åå•
            st.session_state.consultant_settings = edited_df.to_dict('records')
            st.success("é”€å”®äººå‘˜åå•å·²æ›´æ–°ï¼")
    
    with col2:
        if st.button("ğŸ”„ æ¢å¤é»˜è®¤", use_container_width=True, key="reset_consultants"):
            # æ¢å¤é»˜è®¤è®¾ç½®
            st.session_state.consultant_settings = [
                {"å§“å": "é™ˆå©·", "å•ä½": "ä¸Šæµ·å®‰å‰åæµæ±½è½¦æœåŠ¡æœ‰é™å…¬å¸", "æ˜¯å¦å¯ç”¨": True},
                {"å§“å": "å¼ ç†å¹³", "å•ä½": "ä¸Šæµ·å®‰å‰åæµæ±½è½¦æœåŠ¡æœ‰é™å…¬å¸", "æ˜¯å¦å¯ç”¨": True},
                {"å§“å": "é‚µæŒ¯è‰º", "å•ä½": "ä¸Šæµ·å®‰å‰åæµæ±½è½¦æœåŠ¡æœ‰é™å…¬å¸", "æ˜¯å¦å¯ç”¨": True},
                {"å§“å": "è€¿ä½¶", "å•ä½": "ä¸Šæµ·å®‰å‰åæµæ±½è½¦æœåŠ¡æœ‰é™å…¬å¸", "æ˜¯å¦å¯ç”¨": True},
                {"å§“å": "ç¿ä½³è·ƒ", "å•ä½": "å®‰å‰åæµé”€å”®éƒ¨", "æ˜¯å¦å¯ç”¨": False},
                {"å§“å": "é™ˆæ°", "å•ä½": "å®‰å‰åæµé”€å”®éƒ¨", "æ˜¯å¦å¯ç”¨": False}
            ]
            st.success("å·²æ¢å¤é»˜è®¤è®¾ç½®ï¼")

# 4. é”€å”®çº¿ç´¢åˆå¹¶é…ç½®
st.sidebar.subheader("2. åˆå¹¶é…ç½®")

# æ–‡ä»¶ä¸Šä¼ éƒ¨åˆ†
col1, col2 = st.sidebar.columns(2)

with col1:
    yiche_file = st.file_uploader("æ˜“è½¦ç½‘æ–‡ä»¶", type=['xlsx', 'xls'], key="yiche_file")
    
with col2:
    autohome_file = st.file_uploader("æ±½è½¦ä¹‹å®¶æ–‡ä»¶", type=['csv'], key="autohome_file")

# é”€å”®é¡¾é—®é€‰æ‹©
st.sidebar.subheader("é”€å”®é¡¾é—®åˆ†é…")

# ä»è®¾ç½®ä¸­ç”Ÿæˆé”€å”®é¡¾é—®é€‰æ‹©åˆ—è¡¨
consultants = {}
for consultant in st.session_state.consultant_settings:
    consultants[consultant["å§“å"]] = st.sidebar.checkbox(
        f"{consultant['å§“å']} ({consultant['å•ä½']})",
        value=consultant["æ˜¯å¦å¯ç”¨"],
        key=f"consultant_{consultant['å§“å']}"
    )

# è·å–é€‰ä¸­çš„é¡¾é—®åˆ—è¡¨
selected_consultants = [name for name, selected in consultants.items() if selected]

# ç¬¬ä¸€æ¡çº¿ç´¢æŒ‡å®šé¡¾é—® - åŠ¨æ€ç”Ÿæˆé€‰é¡¹
if selected_consultants:
    first_consultant_options = ["è‡ªåŠ¨åˆ†é…"] + selected_consultants
    first_consultant = st.sidebar.selectbox(
        "ç¬¬ä¸€æ¡çº¿ç´¢æŒ‡å®šé¡¾é—®",
        first_consultant_options,
        key="first_consultant"
    )
    
    if first_consultant == "è‡ªåŠ¨åˆ†é…":
        first_consultant = ""
else:
    st.sidebar.warning("è¯·è‡³å°‘é€‰æ‹©ä¸€ä¸ªé”€å”®é¡¾é—®")
    first_consultant = ""

# å¤åˆ¶åŸè„šæœ¬çš„å¤„ç†å‡½æ•°
def remove_after_slash(value):
    """å»é™¤å­—ç¬¦ä¸²ä¸­'/'ä¹‹åçš„å†…å®¹"""
    if pd.isna(value):
        return ""
    value_str = str(value).strip()
    if '/' in value_str:
        return value_str.split('/')[0].strip()
    return value_str

def get_consultant_unit(consultant_name):
    """è·å–é¡¾é—®æ‰€å±å•ä½"""
    # ä»è®¾ç½®ä¸­æŸ¥æ‰¾å•ä½
    for consultant in st.session_state.consultant_settings:
        if consultant["å§“å"] == consultant_name:
            return consultant["å•ä½"]
    return ""

def normalize_car_series(car_series, default_value="æ˜‚ç§‘å¨PLUS", original_source=None):
    """æ ‡å‡†åŒ–è½¦ç³»åç§° - ä½¿ç”¨å¯é…ç½®çš„æ˜ å°„è§„åˆ™"""
    if pd.isna(car_series) or str(car_series).strip() == '':
        return default_value
    
    original = str(car_series).strip()
    
    # ä½¿ç”¨å¯é…ç½®çš„æ˜ å°„è§„åˆ™
    for mapping_rule in st.session_state.car_series_mapping:
        if mapping_rule["æ˜¯å¦å¯ç”¨"]:
            pattern = mapping_rule["åŸå§‹æ¨¡å¼"]
            try:
                if re.search(pattern, original, re.IGNORECASE):
                    return mapping_rule["ç›®æ ‡è½¦ç³»"]
            except re.error:
                # å¦‚æœæ­£åˆ™è¡¨è¾¾å¼æ— æ•ˆï¼Œå°è¯•å­—ç¬¦ä¸²åŒ…å«åŒ¹é…
                if pattern in original or original in pattern:
                    return mapping_rule["ç›®æ ‡è½¦ç³»"]
    
    return default_value

def map_source_category(source_value):
    """æ˜ å°„æ¥æºåˆ†ç±» - ä½¿ç”¨å¯é…ç½®çš„æ˜ å°„è§„åˆ™"""
    if pd.isna(source_value):
        return "å…¶ä»–"
    
    source_str = str(source_value).strip()
    if not source_str:
        return "å…¶ä»–"
    
    # ä½¿ç”¨å¯é…ç½®çš„æ˜ å°„è§„åˆ™
    for mapping_rule in st.session_state.source_category_mapping:
        if mapping_rule["æ˜¯å¦å¯ç”¨"]:
            original_source = mapping_rule["åŸå§‹æ¥æº"]
            # ç²¾ç¡®åŒ¹é…æˆ–æ¨¡ç³ŠåŒ¹é…
            if original_source == source_str or original_source in source_str or source_str in original_source:
                target = mapping_rule["ç›®æ ‡åˆ†ç±»"]
                # å¦‚æœç›®æ ‡åˆ†ç±»ä¸º"æ’é™¤"ï¼Œè¿”å›ç‰¹æ®Šæ ‡è®°
                if target == "æ’é™¤":
                    return "æ’é™¤"
                return target
    
    return "å…¶ä»–"

def map_source_detail(source_value):
    """æ˜ å°„çº¿ç´¢æ¥æº - ä½¿ç”¨å¯é…ç½®çš„æ˜ å°„è§„åˆ™"""
    if pd.isna(source_value):
        return ""
    
    source_str = str(source_value).strip()
    if not source_str:
        return ""
    
    # ä½¿ç”¨å¯é…ç½®çš„æ˜ å°„è§„åˆ™
    for mapping_rule in st.session_state.source_detail_mapping:
        if mapping_rule["æ˜¯å¦å¯ç”¨"]:
            original_source = mapping_rule["åŸå§‹æ¥æº"]
            # ç²¾ç¡®åŒ¹é…æˆ–æ¨¡ç³ŠåŒ¹é…
            if original_source == source_str or original_source in source_str or source_str in original_source:
                target = mapping_rule["ç›®æ ‡çº¿ç´¢æ¥æº"]
                # å¦‚æœç›®æ ‡çº¿ç´¢æ¥æºä¸º"æ’é™¤"ï¼Œè¿”å›ç‰¹æ®Šæ ‡è®°
                if target == "æ’é™¤":
                    return "æ’é™¤"
                return target
    
    return ""

def fair_allocate_consultants(records, selected_consultants_dict, first_consultant=None):
    """å…¬å¹³åˆ†é…é”€å”®é¡¾é—®"""
    # è·å–é€‰ä¸­çš„é¡¾é—®åˆ—è¡¨
    available_consultants = [name for name, selected in selected_consultants_dict.items() if selected]
    
    if not available_consultants:
        return records
    
    # åˆå§‹åŒ–è®¡æ•°å™¨
    consultant_counts = {consultant: 0 for consultant in available_consultants}
    
    # å¦‚æœæŒ‡å®šäº†ç¬¬ä¸€æ¡çº¿ç´¢çš„é¡¾é—®ï¼Œè°ƒæ•´é˜Ÿåˆ—
    if first_consultant and first_consultant in available_consultants:
        first_index = available_consultants.index(first_consultant)
        consultant_queue = available_consultants[first_index:] + available_consultants[:first_index]
    else:
        consultant_queue = available_consultants.copy()
    
    # ä¸ºæ¯æ¡è®°å½•åˆ†é…é¡¾é—®
    for i, record in enumerate(records):
        # é€‰æ‹©åˆ†é…æœ€å°‘çš„é¡¾é—®
        available_counts = {c: consultant_counts[c] for c in consultant_queue}
        min_count = min(available_counts.values())
        min_consultants = [c for c, count in available_counts.items() if count == min_count]
        
        # é€‰æ‹©åœ¨é˜Ÿåˆ—ä¸­ä½ç½®é å‰çš„
        selected_consultant = consultant_queue[0]  # é»˜è®¤é€‰æ‹©ç¬¬ä¸€ä¸ª
        for consultant in consultant_queue:
            if consultant in min_consultants:
                selected_consultant = consultant
                break
        
        # æ›´æ–°åˆ†é…æ•°é‡
        consultant_counts[selected_consultant] += 1
        
        # åˆ†é…é¡¾é—®å’Œå•ä½
        record['é”€å”®é¡¾é—®'] = selected_consultant
        record['å•ä½'] = get_consultant_unit(selected_consultant)
    
    return records

def process_merge(df_yiche, df_autohome, selected_consultants_dict, first_consultant):
    """å¤„ç†åˆå¹¶é€»è¾‘ - æ–°å¢æ’é™¤è§„åˆ™"""
    results = []
    excluded_count = 0
    
    # å¤„ç†æ˜“è½¦ç½‘æ•°æ®
    if df_yiche is not None:
        st.info(f"å¤„ç†æ˜“è½¦ç½‘æ•°æ®: {len(df_yiche)} æ¡è®°å½•")
        for idx, row in df_yiche.iterrows():
            name = remove_after_slash(row.get('å®¢æˆ·å§“å', ''))
            phone = remove_after_slash(row.get('å®¢æˆ·å·ç ', ''))
            
            if pd.isna(name) or pd.isna(phone) or name == '' or phone == '':
                continue
            
            # æ ‡å‡†åŒ–è½¦ç³»
            original_car_series = row.get('çº¿ç´¢æ„å‘è½¦å‹è½¦ç³»', '')
            car_series = normalize_car_series(original_car_series, default_value="æ˜‚ç§‘å¨PLUS", original_source="æ˜“è½¦ç½‘")
            
            # æ¥æºä¿¡æ¯
            source = row.get('å•†ä¸šäº§å“æ¥æº', '')
            if pd.isna(source):
                source = row.get('æ¥æº', '')
            
            source_category = map_source_category(source)
            source_detail = map_source_detail(source)
            
            # æ£€æŸ¥æ˜¯å¦éœ€è¦æ’é™¤ï¼ˆæ¥æºåˆ†ç±»æˆ–çº¿ç´¢æ¥æºä¸º"æ’é™¤"ï¼‰
            if source_category == "æ’é™¤" or source_detail == "æ’é™¤":
                excluded_count += 1
                continue
            
            results.append({
                'å§“å': name,
                'æ‰‹æœºå·': phone,
                'æ€§åˆ«': '',
                'æ¥æºåˆ†ç±»': source_category,
                'çº¿ç´¢æ¥æº': source_detail,
                'å¤‡æ³¨': '',
                'æ„å‘å“ç‰Œ': 'åˆ«å…‹',
                'æ„å‘è½¦ç³»': car_series,
                'é”€å”®é¡¾é—®': '',
                'å•ä½': '',
                'è·Ÿè¿›å†…å®¹': ''
            })
    
    # å¤„ç†æ±½è½¦ä¹‹å®¶æ•°æ®
    if df_autohome is not None:
        st.info(f"å¤„ç†æ±½è½¦ä¹‹å®¶æ•°æ®: {len(df_autohome)} æ¡è®°å½•")
        for idx, row in df_autohome.iterrows():
            name = remove_after_slash(row.get('å®¢æˆ·å§“å', ''))
            phone = remove_after_slash(row.get('å®¢æˆ·æ‰‹æœº', ''))
            
            if pd.isna(name) or pd.isna(phone) or name == '' or phone == '':
                continue
            
            # æ ‡å‡†åŒ–è½¦ç³»
            original_car_series = row.get('æ„å‘è½¦ç³»', '')
            car_series = normalize_car_series(original_car_series, default_value="æ˜‚ç§‘å¨PLUS", original_source="æ±½è½¦ä¹‹å®¶")
            
            # æ¥æºä¿¡æ¯
            bmd_source = row.get('BMDäºŒçº§æ¸ é“', '')
            source_category = map_source_category(bmd_source)
            source_detail = map_source_detail(bmd_source)
            
            # æ£€æŸ¥æ˜¯å¦éœ€è¦æ’é™¤ï¼ˆæ¥æºåˆ†ç±»æˆ–çº¿ç´¢æ¥æºä¸º"æ’é™¤"ï¼‰
            if source_category == "æ’é™¤" or source_detail == "æ’é™¤":
                excluded_count += 1
                continue
            
            results.append({
                'å§“å': name,
                'æ‰‹æœºå·': phone,
                'æ€§åˆ«': '',
                'æ¥æºåˆ†ç±»': source_category,
                'çº¿ç´¢æ¥æº': source_detail,
                'å¤‡æ³¨': '',
                'æ„å‘å“ç‰Œ': 'åˆ«å…‹',
                'æ„å‘è½¦ç³»': car_series,
                'é”€å”®é¡¾é—®': '',
                'å•ä½': '',
                'è·Ÿè¿›å†…å®¹': ''
            })
    
    # åˆå¹¶ç»“æœ
    if excluded_count > 0:
        add_log(f"æ’é™¤äº† {excluded_count} æ¡'ç»é”€å•†å¸‚åœºæ´»åŠ¨'çº¿ç´¢")
    
    if not results:
        st.error("æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆæ•°æ®")
        return None
    
    df = pd.DataFrame(results)
    
    # å»é‡
    before_dedup = len(df)
    df = df.drop_duplicates(subset=['æ‰‹æœºå·'], keep='first')
    after_dedup = len(df)
    
    add_log(f"å»é‡: {before_dedup} -> {after_dedup} æ¡è®°å½•")
    
    # æ£€æŸ¥å¹¶ä¿®å¤ç©ºè½¦ç³»
    empty_car_series_count = df['æ„å‘è½¦ç³»'].isna().sum() + (df['æ„å‘è½¦ç³»'] == '').sum()
    if empty_car_series_count > 0:
        df['æ„å‘è½¦ç³»'] = df['æ„å‘è½¦ç³»'].apply(lambda x: "æ˜‚ç§‘å¨PLUS" if pd.isna(x) or str(x).strip() == '' else x)
    
    # å…¬å¹³åˆ†é…é”€å”®é¡¾é—®
    records = df.to_dict('records')
    records = fair_allocate_consultants(records, selected_consultants_dict, first_consultant)
    
    # è½¬æ¢å›DataFrame
    df = pd.DataFrame(records)
    
    # ç¡®ä¿æ•°æ®åˆ—çš„é¡ºåº
    final_columns = [
        'å§“å', 'æ‰‹æœºå·', 'æ€§åˆ«', 'æ¥æºåˆ†ç±»', 'çº¿ç´¢æ¥æº', 'å¤‡æ³¨',
        'æ„å‘å“ç‰Œ', 'æ„å‘è½¦ç³»', 'é”€å”®é¡¾é—®', 'å•ä½', 'è·Ÿè¿›å†…å®¹'
    ]
    
    # ç¡®ä¿DataFrameåªåŒ…å«æˆ‘ä»¬éœ€è¦çš„åˆ—
    df = df[final_columns]
    
    return df

# ä¸»åŠŸèƒ½åŒº
tab1, tab2, tab3 = st.tabs(["ğŸ“ æ•°æ®ä¸Šä¼ ", "âš™ï¸ æ•°æ®å¤„ç†", "ğŸ“Š ç»“æœåˆ†æ"])

with tab1:
    st.header("æ•°æ®æ–‡ä»¶ä¸Šä¼ ")
    
    col1, col2 = st.columns(2)
    
    with col1:
        st.subheader("æ˜“è½¦ç½‘æ–‡ä»¶")
        if yiche_file:
            try:
                df_yiche = pd.read_excel(yiche_file)
                st.success(f"âœ… æˆåŠŸè¯»å–æ˜“è½¦ç½‘æ–‡ä»¶ï¼Œå…± {len(df_yiche)} æ¡è®°å½•")
                st.dataframe(df_yiche.head(), use_container_width=True)
                
                # æ˜¾ç¤ºåˆ—å
                with st.expander("æŸ¥çœ‹æ–‡ä»¶åˆ—å"):
                    st.write("åˆ—ååˆ—è¡¨:", list(df_yiche.columns))
            except Exception as e:
                st.error(f"è¯»å–å¤±è´¥: {str(e)}")
        else:
            st.info("è¯·ä¸Šä¼ æ˜“è½¦ç½‘Excelæ–‡ä»¶")
    
    with col2:
        st.subheader("æ±½è½¦ä¹‹å®¶æ–‡ä»¶")
        if autohome_file:
            try:
                # å°è¯•å¤šç§ç¼–ç è¯»å–
                try:
                    df_autohome = pd.read_csv(autohome_file, encoding='utf-8')
                except:
                    try:
                        df_autohome = pd.read_csv(autohome_file, encoding='gbk')
                    except:
                        # å¦‚æœéƒ½ä¸è¡Œï¼Œå°è¯•è¯»å–åŸå§‹å­—èŠ‚å¹¶æ‰‹åŠ¨å¤„ç†
                        content = autohome_file.getvalue()
                        try:
                            content_str = content.decode('utf-8-sig')
                        except:
                            content_str = content.decode('utf-8', errors='ignore')
                        
                        # ä½¿ç”¨StringIOåŒ…è£…
                        df_autohome = pd.read_csv(io.StringIO(content_str))
                
                st.success(f"âœ… æˆåŠŸè¯»å–æ±½è½¦ä¹‹å®¶æ–‡ä»¶ï¼Œå…± {len(df_autohome)} æ¡è®°å½•")
                st.dataframe(df_autohome.head(), use_container_width=True)
                
                # æ˜¾ç¤ºåˆ—å
                with st.expander("æŸ¥çœ‹æ–‡ä»¶åˆ—å"):
                    st.write("åˆ—ååˆ—è¡¨:", list(df_autohome.columns))
                    
            except Exception as e:
                st.error(f"è¯»å–å¤±è´¥: {str(e)}")
                st.info("å»ºè®®å…ˆä½¿ç”¨å·¦ä¾§çš„'æ–‡ä»¶æ ¼å¼ä¿®å¤'åŠŸèƒ½å¤„ç†æ­¤æ–‡ä»¶")
        else:
            st.info("è¯·ä¸Šä¼ æ±½è½¦ä¹‹å®¶CSVæ–‡ä»¶")
    
    # æ˜¾ç¤ºä¿®å¤åçš„æ•°æ®ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
    if st.session_state.fixed_autohome_df is not None:
        st.subheader("ä¿®å¤åçš„æ±½è½¦ä¹‹å®¶æ•°æ®")
        st.dataframe(st.session_state.fixed_autohome_df.head(), use_container_width=True)
        st.success(f"ä¿®å¤åçš„æ•°æ®å…± {len(st.session_state.fixed_autohome_df)} æ¡è®°å½•")

with tab2:
    st.header("æ•°æ®å¤„ç†")
    
    # æ˜¾ç¤ºå½“å‰æ˜ å°„è§„åˆ™ç»Ÿè®¡
    with st.expander("æŸ¥çœ‹å½“å‰æ˜ å°„è§„åˆ™ç»Ÿè®¡", expanded=False):
        col1, col2, col3 = st.columns(3)
        
        with col1:
            enabled_car = sum(1 for rule in st.session_state.car_series_mapping if rule["æ˜¯å¦å¯ç”¨"])
            total_car = len(st.session_state.car_series_mapping)
            st.metric("è½¦ç³»æ˜ å°„è§„åˆ™", f"{enabled_car}/{total_car} æ¡å¯ç”¨")
        
        with col2:
            enabled_category = sum(1 for rule in st.session_state.source_category_mapping if rule["æ˜¯å¦å¯ç”¨"])
            total_category = len(st.session_state.source_category_mapping)
            st.metric("æ¥æºåˆ†ç±»è§„åˆ™", f"{enabled_category}/{total_category} æ¡å¯ç”¨")
        
        with col3:
            enabled_detail = sum(1 for rule in st.session_state.source_detail_mapping if rule["æ˜¯å¦å¯ç”¨"])
            total_detail = len(st.session_state.source_detail_mapping)
            st.metric("çº¿ç´¢æ¥æºè§„åˆ™", f"{enabled_detail}/{total_detail} æ¡å¯ç”¨")
    
    # æ£€æŸ¥æ˜¯å¦æœ‰é€‰ä¸­çš„é”€å”®é¡¾é—®
    if not selected_consultants:
        st.warning("âš ï¸ è¯·å…ˆåœ¨ä¾§è¾¹æ é€‰æ‹©è‡³å°‘ä¸€ä¸ªé”€å”®é¡¾é—®")
    
    # æ£€æŸ¥æ˜¯å¦ä¸Šä¼ äº†æ–‡ä»¶
    files_available = (yiche_file is not None) or (autohome_file is not None) or (st.session_state.fixed_autohome_df is not None)
    
    if files_available and selected_consultants:
        if st.button("ğŸš€ å¼€å§‹åˆå¹¶å¤„ç†", type="primary"):
            with st.spinner("æ­£åœ¨å¤„ç†æ•°æ®..."):
                try:
                    # è¯»å–æ•°æ®
                    df_yiche = None
                    df_autohome = None
                    
                    if yiche_file:
                        df_yiche = pd.read_excel(yiche_file)
                    
                    # ä¼˜å…ˆä½¿ç”¨ä¿®å¤åçš„æ•°æ®
                    if st.session_state.fixed_autohome_df is not None:
                        df_autohome = st.session_state.fixed_autohome_df
                    elif autohome_file:
                        try:
                            df_autohome = pd.read_csv(autohome_file, encoding='utf-8')
                        except:
                            try:
                                df_autohome = pd.read_csv(autohome_file, encoding='gbk')
                            except:
                                content = autohome_file.getvalue()
                                content_str = content.decode('utf-8', errors='ignore')
                                df_autohome = pd.read_csv(io.StringIO(content_str))
                    
                    # å¤„ç†åˆå¹¶
                    if df_yiche is not None or df_autohome is not None:
                        df_result = process_merge(df_yiche, df_autohome, consultants, first_consultant)
                        
                        if df_result is not None:
                            # ä¿å­˜åˆ°session state
                            st.session_state.df_merged = df_result
                            
                            # æ˜¾ç¤ºå¤„ç†æ—¥å¿—
                            st.success(f"âœ… æ•°æ®å¤„ç†å®Œæˆï¼å…±åˆå¹¶ {len(df_result)} æ¡è®°å½•")
                            
                            # æ˜¾ç¤ºåˆ†é…ç»Ÿè®¡
                            st.subheader("é”€å”®é¡¾é—®åˆ†é…ç»Ÿè®¡")
                            allocation_counts = {}
                            for consultant in selected_consultants:
                                count = len(df_result[df_result['é”€å”®é¡¾é—®'] == consultant])
                                allocation_counts[consultant] = count
                                
                                # è·å–å•ä½
                                unit = get_consultant_unit(consultant)
                                st.write(f"**{consultant}** ({unit}): {count}æ¡")
                            
                            # æ£€æŸ¥åˆ†é…å‡åŒ€åº¦
                            counts = list(allocation_counts.values())
                            if counts:
                                max_count = max(counts)
                                min_count = min(counts)
                                if max_count - min_count > 1:
                                    st.warning(f"âš ï¸ åˆ†é…ä¸å‡åŒ€ï¼Œæœ€å¤§å·®å€¼ {max_count - min_count}")
                                else:
                                    st.success(f"âœ“ åˆ†é…å‡åŒ€ï¼Œæœ€å¤§å·®å€¼ {max_count - min_count}")
                    else:
                        st.error("æ²¡æœ‰å¯å¤„ç†çš„æ•°æ®æ–‡ä»¶")
                        
                except Exception as e:
                    st.error(f"å¤„ç†å¤±è´¥: {str(e)}")
                    st.info("é”™è¯¯è¯¦æƒ…:")
                    st.code(str(e))
    else:
        st.info("è¯·å…ˆä¸Šä¼ éœ€è¦å¤„ç†çš„æ–‡ä»¶ï¼Œå¹¶ç¡®ä¿å·²é€‰æ‹©è‡³å°‘ä¸€ä¸ªé”€å”®é¡¾é—®")

with tab3:
    st.header("ç»“æœåˆ†æä¸ä¸‹è½½")
    
    if st.session_state.df_merged is not None:
        df = st.session_state.df_merged
        
        # æ˜¾ç¤ºæ•°æ®é¢„è§ˆ
        st.subheader("ğŸ“‹ æ•°æ®é¢„è§ˆ")
        st.dataframe(df.head(20), use_container_width=True)
        
        # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
        col1, col2 = st.columns(2)
        
        with col1:
            st.subheader("ğŸ“Š åŸºæœ¬ç»Ÿè®¡")
            st.metric("æ€»è®°å½•æ•°", len(df))
            
            # è½¦ç³»ç»Ÿè®¡
            car_stats = df['æ„å‘è½¦ç³»'].value_counts()
            st.metric("è½¦å‹ç§ç±»", len(car_stats))
            
            # æ¥æºç»Ÿè®¡
            source_stats = df['çº¿ç´¢æ¥æº'].value_counts()
            st.metric("æ¥æºæ¸ é“", len(source_stats))
            
            # æ˜¾ç¤ºè½¦ç³»ç»Ÿè®¡è¯¦æƒ…
            with st.expander("æŸ¥çœ‹è½¦ç³»ç»Ÿè®¡è¯¦æƒ…"):
                for car, count in car_stats.items():
                    st.write(f"{car}: {count}æ¡")
            
        with col2:
            st.subheader("ğŸ” è½¦ç³»ç»Ÿè®¡å›¾è¡¨")
            if not df['æ„å‘è½¦ç³»'].empty:
                car_stats = df['æ„å‘è½¦ç³»'].value_counts()
                st.bar_chart(car_stats)
            
            # æ˜¾ç¤ºå‰5å¤§è½¦å‹
            st.subheader("ğŸ† å‰5å¤§è½¦å‹")
            top5 = car_stats.head(5)
            for i, (car, count) in enumerate(top5.items(), 1):
                percentage = (count / len(df)) * 100
                st.write(f"{i}. {car}: {count}æ¡ ({percentage:.1f}%)")
        
        # æ˜¾ç¤ºçº¿ç´¢æ¥æºç»Ÿè®¡
        st.subheader("ğŸ“ˆ çº¿ç´¢æ¥æºç»Ÿè®¡")
        col3, col4 = st.columns(2)
        
        with col3:
            if not df['çº¿ç´¢æ¥æº'].empty:
                source_stats = df['çº¿ç´¢æ¥æº'].value_counts()
                st.bar_chart(source_stats)
        
        with col4:
            with st.expander("æŸ¥çœ‹æ¥æºç»Ÿè®¡è¯¦æƒ…"):
                for source, count in source_stats.items():
                    st.write(f"{source}: {count}æ¡")
        
        # æ˜¾ç¤ºé”€å”®é¡¾é—®ç»Ÿè®¡
        st.subheader("ğŸ‘¥ é”€å”®é¡¾é—®ç»Ÿè®¡")
        consultant_stats = df['é”€å”®é¡¾é—®'].value_counts()
        col5, col6 = st.columns(2)
        
        with col5:
            st.bar_chart(consultant_stats)
        
        with col6:
            with st.expander("æŸ¥çœ‹é”€å”®é¡¾é—®ç»Ÿè®¡è¯¦æƒ…"):
                for consultant, count in consultant_stats.items():
                    unit = df[df['é”€å”®é¡¾é—®'] == consultant]['å•ä½'].iloc[0] if len(df[df['é”€å”®é¡¾é—®'] == consultant]) > 0 else ""
                    st.write(f"{consultant} ({unit}): {count}æ¡")
        
        # æä¾›ä¸‹è½½
        st.subheader("ğŸ’¾ ä¸‹è½½ç»“æœ")
        
        # è½¬æ¢ä¸ºExcel
        output = io.BytesIO()
        with pd.ExcelWriter(output, engine='openpyxl') as writer:
            df.to_excel(writer, index=False, sheet_name='åˆå¹¶ç»“æœ')
        output.seek(0)
        
        st.download_button(
            label="ğŸ“¥ ä¸‹è½½Excelæ–‡ä»¶",
            data=output,
            file_name=f"CRSçº¿ç´¢_{datetime.now().strftime('%Y%m%d_%H%M%S')}.xlsx",
            mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
            type="primary"
        )
        
        # æä¾›CSVæ ¼å¼ä¸‹è½½
        csv_output = df.to_csv(index=False, encoding='utf-8-sig')
        st.download_button(
            label="ğŸ“„ ä¸‹è½½CSVæ–‡ä»¶",
            data=csv_output,
            file_name=f"CRSçº¿ç´¢_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv",
            mime="text/csv"
        )
    else:
        st.info("è¯·å…ˆå¤„ç†æ•°æ®ä»¥æŸ¥çœ‹ç»“æœ")

# é¡µè„š
st.markdown("---")
st.caption("é”€å”®çº¿ç´¢åˆå¹¶å·¥å…· v0.5.1 | æ”¯æŒè‡ªå®šä¹‰æ˜ å°„è§„åˆ™å’Œæ’é™¤è§„åˆ™ | All by Eric & deepseek")