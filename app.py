import streamlit as st
import pandas as pd
import numpy as np
import re
import io
from datetime import datetime
from collections import defaultdict
import tempfile
import os

# è®¾ç½®é¡µé¢é…ç½®
st.set_page_config(
    page_title="é”€å”®çº¿ç´¢åˆå¹¶å·¥å…·",
    page_icon="ğŸ“Š",
    layout="wide",
    initial_sidebar_state="expanded"
)

# è®¾ç½®åº”ç”¨æ ‡é¢˜å’Œè¯´æ˜
st.title("ğŸ“Š é”€å”®çº¿ç´¢åˆå¹¶å·¥å…· - Webç‰ˆ")
st.markdown("---")

# åˆå§‹åŒ– session state
if 'df_merged' not in st.session_state:
    st.session_state.df_merged = None
if 'processing_log' not in st.session_state:
    st.session_state.processing_log = []
if 'fixed_autohome_df' not in st.session_state:
    st.session_state.fixed_autohome_df = None

def add_log(message):
    """æ·»åŠ å¤„ç†æ—¥å¿—"""
    st.session_state.processing_log.append(f"{datetime.now().strftime('%H:%M:%S')} - {message}")

# ä¾§è¾¹æ é…ç½®
st.sidebar.header("âš™ï¸ é…ç½®é€‰é¡¹")

# 1. æ–‡ä»¶æ ¼å¼ä¿®å¤éƒ¨åˆ†ï¼ˆåŸ"æ”¹æ–‡ä»¶æ ¼å¼.ipynb"çš„åŠŸèƒ½ï¼‰
st.sidebar.subheader("1. æ–‡ä»¶æ ¼å¼ä¿®å¤")
uploaded_file = st.sidebar.file_uploader(
    "ä¸Šä¼ æ±½è½¦ä¹‹å®¶CSVæ–‡ä»¶",
    type=['csv'],
    help="ä¸Šä¼ éœ€è¦ä¿®å¤æ ¼å¼çš„æ±½è½¦ä¹‹å®¶CSVæ–‡ä»¶",
    key="autohome_original"
)

if uploaded_file is not None:
    st.sidebar.success(f"å·²ä¸Šä¼ : {uploaded_file.name}")
    
    # ä¿®å¤CSVæ ¼å¼çš„å‡½æ•°
    def fix_csv_format(file_content):
        """ä¿®å¤CSVæ ¼å¼é—®é¢˜"""
        try:
            # å°è¯•å¤šç§æ–¹å¼è¯»å–
            content = file_content.decode('utf-8-sig')
        except:
            try:
                content = file_content.decode('gbk')
            except:
                content = file_content.decode('utf-8', errors='ignore')
        
        lines = content.splitlines()
        processed_lines = []
        
        for i, line in enumerate(lines):
            line = line.strip()
            
            if i == 0:
                # æ ‡é¢˜è¡Œ
                processed_lines.append(line)
                continue
            
            if not line:  # è·³è¿‡ç©ºè¡Œ
                continue
            
            # å¤„ç†å¼•å·
            if line.startswith('"') and line.endswith('"'):
                line = line[1:-1]
            
            # å¤„ç†è½¬ä¹‰çš„åŒå¼•å·
            line = line.replace('""', 'TEMP_QUOTE')
            
            # åˆ†å‰²å­—æ®µ
            parts = []
            current_part = []
            in_quotes = False
            
            for char in line:
                if char == '"' and not in_quotes:
                    in_quotes = True
                elif char == '"' and in_quotes:
                    in_quotes = False
                elif char == ',' and not in_quotes:
                    parts.append(''.join(current_part))
                    current_part = []
                else:
                    current_part.append(char)
            
            if current_part:
                parts.append(''.join(current_part))
            
            # æ¢å¤è½¬ä¹‰çš„åŒå¼•å·
            processed_parts = []
            for part in parts:
                if part.startswith('"') and part.endswith('"'):
                    part = part[1:-1]
                part = part.replace('TEMP_QUOTE', '"')
                processed_parts.append(part)
            
            processed_lines.append(','.join(processed_parts))
        
        return '\n'.join(processed_lines)
    
    # æ˜¾ç¤ºä¿®å¤é€‰é¡¹
    if st.sidebar.button("ä¿®å¤æ–‡ä»¶æ ¼å¼"):
        try:
            fixed_content = fix_csv_format(uploaded_file.getvalue())
            
            # è¯»å–ä¿®å¤åçš„å†…å®¹åˆ°DataFrame
            try:
                fixed_df = pd.read_csv(io.StringIO(fixed_content))
                st.session_state.fixed_autohome_df = fixed_df
                
                # æä¾›ä¸‹è½½ä¿®å¤åçš„æ–‡ä»¶
                st.sidebar.download_button(
                    label="ä¸‹è½½ä¿®å¤åçš„æ–‡ä»¶",
                    data=fixed_content,
                    file_name=f"fixed_{uploaded_file.name}",
                    mime="text/csv"
                )
                st.sidebar.success(f"æ–‡ä»¶æ ¼å¼ä¿®å¤å®Œæˆï¼å…± {len(fixed_df)} æ¡è®°å½•")
                
                # æ˜¾ç¤ºä¿®å¤åçš„æ•°æ®é¢„è§ˆ
                with st.sidebar.expander("æŸ¥çœ‹ä¿®å¤åçš„æ•°æ®é¢„è§ˆ"):
                    st.dataframe(fixed_df.head(5))
                    
            except Exception as e:
                st.sidebar.error(f"è¯»å–ä¿®å¤åçš„æ–‡ä»¶å¤±è´¥: {str(e)}")
                
        except Exception as e:
            st.sidebar.error(f"ä¿®å¤å¤±è´¥: {str(e)}")

# 2. é”€å”®çº¿ç´¢åˆå¹¶é…ç½®
st.sidebar.subheader("2. åˆå¹¶é…ç½®")

# æ–‡ä»¶ä¸Šä¼ éƒ¨åˆ†
col1, col2 = st.sidebar.columns(2)

with col1:
    yiche_file = st.file_uploader("æ˜“è½¦ç½‘æ–‡ä»¶", type=['xlsx', 'xls'], key="yiche_file")
    
with col2:
    autohome_file = st.file_uploader("æ±½è½¦ä¹‹å®¶æ–‡ä»¶", type=['csv'], key="autohome_file")

# é”€å”®é¡¾é—®é€‰æ‹©
st.sidebar.subheader("é”€å”®é¡¾é—®åˆ†é…")
consultants = {
    "é™ˆå©·": st.sidebar.checkbox("é™ˆå©·", value=True, key="chen_ting"),
    "å¼ ç†å¹³": st.sidebar.checkbox("å¼ ç†å¹³", value=True, key="zhang_liping"),
    "é‚µæŒ¯è‰º": st.sidebar.checkbox("é‚µæŒ¯è‰º", value=True, key="shao_zhenyi"),
    "è€¿ä½¶": st.sidebar.checkbox("è€¿ä½¶", value=True, key="geng_ji"),
    "ç¿ä½³è·ƒ": st.sidebar.checkbox("ç¿ä½³è·ƒ", value=False, key="weng_jiayue"),
    "é™ˆæ°": st.sidebar.checkbox("é™ˆæ°", value=False, key="chen_jie")
}

# è·å–é€‰ä¸­çš„é¡¾é—®åˆ—è¡¨
selected_consultants = [name for name, selected in consultants.items() if selected]

# ç¬¬ä¸€æ¡çº¿ç´¢æŒ‡å®šé¡¾é—® - åŠ¨æ€ç”Ÿæˆé€‰é¡¹
if selected_consultants:
    first_consultant_options = ["è‡ªåŠ¨åˆ†é…"] + selected_consultants
    first_consultant = st.sidebar.selectbox(
        "ç¬¬ä¸€æ¡çº¿ç´¢æŒ‡å®šé¡¾é—®",
        first_consultant_options,
        key="first_consultant"
    )
    
    if first_consultant == "è‡ªåŠ¨åˆ†é…":
        first_consultant = ""
else:
    st.sidebar.warning("è¯·è‡³å°‘é€‰æ‹©ä¸€ä¸ªé”€å”®é¡¾é—®")
    first_consultant = ""

# è½¦ç³»åç§°æ˜ å°„è§„åˆ™ï¼ˆä»åŸè„šæœ¬å¤åˆ¶ï¼‰
car_series_mapping = {
    r".*GL8.*é™†å°Š.*": "GL8 è±ªåå•†åŠ¡è½¦",
    r".*GL8.*é™†ä¸Šå…¬åŠ¡èˆ±.*": "GL8 é™†ä¸Šå…¬åŠ¡èˆ±",
    r".*GL8.*é™†å°š.*": "GL8é™†å°š",
    r".*GL8.*Avenir.*": "GL8 Avenir",
    r".*GL8.*è±ªåå•†åŠ¡è½¦.*": "GL8 è±ªåå•†åŠ¡è½¦",
    r".*å›è¶Š.*": "å…¨æ–°ä¸€ä»£å›è¶Š",
    r".*å›å¨.*": "å…¨æ–°ä¸€ä»£å›å¨",
    r".*æ–°å›å¨.*": "å…¨æ–°ä¸€ä»£å›å¨",
    r".*æ˜‚ç§‘å¨Plus.*": "æ˜‚ç§‘å¨PLUS",
    r".*æ˜‚ç§‘å¨PLUS.*": "æ˜‚ç§‘å¨PLUS",
    r".*æ˜‚ç§‘å¨S.*": "æ˜‚ç§‘å¨S",
    r".*å¨æœ—.*": "å¨æœ—Pro",
    r".*å¾®è“6.*": "VELITE 6",
    r".*VELITE 6.*": "VELITE 6",
    r".*E5.*": "E 5",
    r".*E 5.*": "E 5",
    r".*ä¸–çºª.*": "ä¸–çºª",
    r".*è‡³å¢ƒ.*": "è‡³å¢ƒä¸–å®¶",
    r".*æ˜‚ç§‘æ——.*": "æ˜‚ç§‘å¨PLUS",
    r".*åˆ«å…‹.*": "æ˜‚ç§‘å¨PLUS",
}

source_category_mapping = {
    "è½¦å•†æ±‡": "å‚åª’", "è½¦å•†æ±‡(é›†å®¢å·)": "å‚åª’", "è½¦å•†æ±‡ï¼ˆIMä¼šè¯ï¼‰": "å‚åª’", "è½¦å•†æ±‡ï¼ˆåˆ†æœŸï¼‰": "å‚åª’", "è½¦å•†æ±‡ï¼ˆå¹³å°æ´»åŠ¨ï¼‰": "å‚åª’",
    "æ™ºèƒ½äº§å“ï¼ˆæ™ºèƒ½å±•å…ï¼‰": "å‚åª’", "æŠ–éŸ³": "è‡ªåª’", "æœ¬åœ°é€š-ç»é”€å•†å·": "è‡ªåª’", "æœ¬åœ°é€šå¼‚åœ°-ç»é”€å•†å·": "è‡ªåª’",
    "æ˜“è½¦ç½‘": "å‚åª’", "æ±½è½¦ä¹‹å®¶": "å‚åª’"
}

source_detail_mapping = {
    "è½¦å•†æ±‡": "æ±½è½¦ä¹‹å®¶", "è½¦å•†æ±‡(é›†å®¢å·)": "æ±½è½¦ä¹‹å®¶", "è½¦å•†æ±‡ï¼ˆIMä¼šè¯ï¼‰": "æ±½è½¦ä¹‹å®¶", "è½¦å•†æ±‡ï¼ˆåˆ†æœŸï¼‰": "æ±½è½¦ä¹‹å®¶", "è½¦å•†æ±‡ï¼ˆå¹³å°æ´»åŠ¨ï¼‰": "æ±½è½¦ä¹‹å®¶",
    "æ™ºèƒ½äº§å“ï¼ˆæ™ºèƒ½å±•å…ï¼‰": "æ±½è½¦ä¹‹å®¶", "æŠ–éŸ³": "æŠ–éŸ³", "æœ¬åœ°é€š-ç»é”€å•†å·": "æŠ–éŸ³", "æœ¬åœ°é€šå¼‚åœ°-ç»é”€å•†å·": "æŠ–éŸ³",
    "æ˜“è½¦ç½‘": "æ˜“è½¦", "æ±½è½¦ä¹‹å®¶": "æ±½è½¦ä¹‹å®¶"
}

# å¤åˆ¶åŸè„šæœ¬çš„å¤„ç†å‡½æ•°
def remove_after_slash(value):
    """å»é™¤å­—ç¬¦ä¸²ä¸­'/'ä¹‹åçš„å†…å®¹"""
    if pd.isna(value):
        return ""
    value_str = str(value).strip()
    if '/' in value_str:
        return value_str.split('/')[0].strip()
    return value_str

def get_consultant_unit(consultant_name):
    """è·å–é¡¾é—®æ‰€å±å•ä½"""
    if consultant_name in ["å¼ ç†å¹³", "é‚µæŒ¯è‰º", "è€¿ä½¶", "é™ˆå©·"]:
        return "ä¸Šæµ·å®‰å‰åæµæ±½è½¦æœåŠ¡æœ‰é™å…¬å¸"
    elif consultant_name in ["ç¿ä½³è·ƒ", "é™ˆæ°"]:
        return "å®‰å‰åæµé”€å”®éƒ¨"
    return ""

def normalize_car_series(car_series, default_value="æ˜‚ç§‘å¨PLUS", original_source=None):
    """æ ‡å‡†åŒ–è½¦ç³»åç§°"""
    if pd.isna(car_series) or str(car_series).strip() == '':
        return default_value
    
    original = str(car_series).strip()
    
    for pattern, replacement in car_series_mapping.items():
        if re.search(pattern, original, re.IGNORECASE):
            return replacement
    
    return default_value

def map_source(source_value, mapping_dict, field_name="æ¥æº"):
    """æ˜ å°„æ¥æºå­—æ®µ"""
    if pd.isna(source_value):
        return "å…¶ä»–"
    
    source_str = str(source_value).strip()
    if not source_str:
        return "å…¶ä»–"
    
    # å°è¯•ç²¾ç¡®åŒ¹é…
    if source_str in mapping_dict:
        return mapping_dict[source_str]
    
    # å°è¯•æ¨¡ç³ŠåŒ¹é…
    for key, value in mapping_dict.items():
        if key in source_str or source_str in key:
            return value
    
    return "å…¶ä»–"

def fair_allocate_consultants(records, selected_consultants_dict, first_consultant=None):
    """å…¬å¹³åˆ†é…é”€å”®é¡¾é—®"""
    # è·å–é€‰ä¸­çš„é¡¾é—®åˆ—è¡¨
    available_consultants = [name for name, selected in selected_consultants_dict.items() if selected]
    
    if not available_consultants:
        return records
    
    # åˆå§‹åŒ–è®¡æ•°å™¨
    consultant_counts = {consultant: 0 for consultant in available_consultants}
    
    # å¦‚æœæŒ‡å®šäº†ç¬¬ä¸€æ¡çº¿ç´¢çš„é¡¾é—®ï¼Œè°ƒæ•´é˜Ÿåˆ—
    if first_consultant and first_consultant in available_consultants:
        first_index = available_consultants.index(first_consultant)
        consultant_queue = available_consultants[first_index:] + available_consultants[:first_index]
    else:
        consultant_queue = available_consultants.copy()
    
    # ä¸ºæ¯æ¡è®°å½•åˆ†é…é¡¾é—®
    for i, record in enumerate(records):
        # é€‰æ‹©åˆ†é…æœ€å°‘çš„é¡¾é—®
        available_counts = {c: consultant_counts[c] for c in consultant_queue}
        min_count = min(available_counts.values())
        min_consultants = [c for c, count in available_counts.items() if count == min_count]
        
        # é€‰æ‹©åœ¨é˜Ÿåˆ—ä¸­ä½ç½®é å‰çš„
        selected_consultant = consultant_queue[0]  # é»˜è®¤é€‰æ‹©ç¬¬ä¸€ä¸ª
        for consultant in consultant_queue:
            if consultant in min_consultants:
                selected_consultant = consultant
                break
        
        # æ›´æ–°åˆ†é…æ•°é‡
        consultant_counts[selected_consultant] += 1
        
        # åˆ†é…é¡¾é—®å’Œå•ä½
        record['é”€å”®é¡¾é—®'] = selected_consultant
        record['å•ä½'] = get_consultant_unit(selected_consultant)
    
    return records

def process_merge(df_yiche, df_autohome, selected_consultants_dict, first_consultant):
    """å¤„ç†åˆå¹¶é€»è¾‘"""
    results = []
    
    # å¤„ç†æ˜“è½¦ç½‘æ•°æ®
    if df_yiche is not None:
        st.info(f"å¤„ç†æ˜“è½¦ç½‘æ•°æ®: {len(df_yiche)} æ¡è®°å½•")
        for idx, row in df_yiche.iterrows():
            name = remove_after_slash(row.get('å®¢æˆ·å§“å', ''))
            phone = remove_after_slash(row.get('å®¢æˆ·å·ç ', ''))
            
            if pd.isna(name) or pd.isna(phone) or name == '' or phone == '':
                continue
            
            # æ ‡å‡†åŒ–è½¦ç³»
            original_car_series = row.get('çº¿ç´¢æ„å‘è½¦å‹è½¦ç³»', '')
            car_series = normalize_car_series(original_car_series, default_value="æ˜‚ç§‘å¨PLUS", original_source="æ˜“è½¦ç½‘")
            
            # æ¥æºä¿¡æ¯
            source = row.get('å•†ä¸šäº§å“æ¥æº', '')
            if pd.isna(source):
                source = row.get('æ¥æº', '')
            
            source_category = map_source(source, source_category_mapping, "æ¥æºåˆ†ç±»")
            source_detail = map_source(source, source_detail_mapping, "çº¿ç´¢æ¥æº")
            
            results.append({
                'å§“å': name,
                'æ‰‹æœºå·': phone,
                'æ€§åˆ«': '',
                'æ¥æºåˆ†ç±»': source_category,
                'çº¿ç´¢æ¥æº': source_detail,
                'å¤‡æ³¨': '',
                'æ„å‘å“ç‰Œ': 'åˆ«å…‹',
                'æ„å‘è½¦ç³»': car_series,
                'é”€å”®é¡¾é—®': '',
                'å•ä½': '',
                'è·Ÿè¿›å†…å®¹': '',
                # æ³¨é‡Šæ‰åŸå§‹è½¦ç³»å’ŒåŸå§‹æ¥æºå­—æ®µ
                # 'åŸå§‹è½¦ç³»': str(original_car_series),
                # 'åŸå§‹æ¥æº': str(source)
            })
    
    # å¤„ç†æ±½è½¦ä¹‹å®¶æ•°æ®
    if df_autohome is not None:
        st.info(f"å¤„ç†æ±½è½¦ä¹‹å®¶æ•°æ®: {len(df_autohome)} æ¡è®°å½•")
        for idx, row in df_autohome.iterrows():
            name = remove_after_slash(row.get('å®¢æˆ·å§“å', ''))
            phone = remove_after_slash(row.get('å®¢æˆ·æ‰‹æœº', ''))
            
            if pd.isna(name) or pd.isna(phone) or name == '' or phone == '':
                continue
            
            # æ ‡å‡†åŒ–è½¦ç³»
            original_car_series = row.get('æ„å‘è½¦ç³»', '')
            car_series = normalize_car_series(original_car_series, default_value="æ˜‚ç§‘å¨PLUS", original_source="æ±½è½¦ä¹‹å®¶")
            
            # æ¥æºä¿¡æ¯
            bmd_source = row.get('BMDäºŒçº§æ¸ é“', '')
            source_category = map_source(bmd_source, source_category_mapping, "æ¥æºåˆ†ç±»")
            source_detail = map_source(bmd_source, source_detail_mapping, "çº¿ç´¢æ¥æº")
            
            results.append({
                'å§“å': name,
                'æ‰‹æœºå·': phone,
                'æ€§åˆ«': '',
                'æ¥æºåˆ†ç±»': source_category,
                'çº¿ç´¢æ¥æº': source_detail,
                'å¤‡æ³¨': '',
                'æ„å‘å“ç‰Œ': 'åˆ«å…‹',
                'æ„å‘è½¦ç³»': car_series,
                'é”€å”®é¡¾é—®': '',
                'å•ä½': '',
                'è·Ÿè¿›å†…å®¹': '',
                # æ³¨é‡Šæ‰åŸå§‹è½¦ç³»å’ŒåŸå§‹æ¥æºå­—æ®µ
                # 'åŸå§‹è½¦ç³»': str(original_car_series),
                # 'åŸå§‹æ¥æº': str(bmd_source)
            })
    
    # åˆå¹¶ç»“æœ
    if not results:
        st.error("æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆæ•°æ®")
        return None
    
    df = pd.DataFrame(results)
    
    # å»é‡
    before_dedup = len(df)
    df = df.drop_duplicates(subset=['æ‰‹æœºå·'], keep='first')
    after_dedup = len(df)
    
    add_log(f"å»é‡: {before_dedup} -> {after_dedup} æ¡è®°å½•")
    
    # æ£€æŸ¥å¹¶ä¿®å¤ç©ºè½¦ç³»
    empty_car_series_count = df['æ„å‘è½¦ç³»'].isna().sum() + (df['æ„å‘è½¦ç³»'] == '').sum()
    if empty_car_series_count > 0:
        df['æ„å‘è½¦ç³»'] = df['æ„å‘è½¦ç³»'].apply(lambda x: "æ˜‚ç§‘å¨PLUS" if pd.isna(x) or str(x).strip() == '' else x)
    
    # å…¬å¹³åˆ†é…é”€å”®é¡¾é—®
    records = df.to_dict('records')
    records = fair_allocate_consultants(records, selected_consultants_dict, first_consultant)
    
    # è½¬æ¢å›DataFrame
    df = pd.DataFrame(records)
    
    # ç¡®ä¿æ•°æ®åˆ—çš„é¡ºåºï¼ˆå»é™¤ä¸éœ€è¦çš„åˆ—ï¼‰
    final_columns = [
        'å§“å', 'æ‰‹æœºå·', 'æ€§åˆ«', 'æ¥æºåˆ†ç±»', 'çº¿ç´¢æ¥æº', 'å¤‡æ³¨',
        'æ„å‘å“ç‰Œ', 'æ„å‘è½¦ç³»', 'é”€å”®é¡¾é—®', 'å•ä½', 'è·Ÿè¿›å†…å®¹'
    ]
    
    # ç¡®ä¿DataFrameåªåŒ…å«æˆ‘ä»¬éœ€è¦çš„åˆ—
    df = df[final_columns]
    
    return df

# ä¸»åŠŸèƒ½åŒº
tab1, tab2, tab3 = st.tabs(["ğŸ“ æ•°æ®ä¸Šä¼ ", "âš™ï¸ æ•°æ®å¤„ç†", "ğŸ“Š ç»“æœåˆ†æ"])

with tab1:
    st.header("æ•°æ®æ–‡ä»¶ä¸Šä¼ ")
    
    col1, col2 = st.columns(2)
    
    with col1:
        st.subheader("æ˜“è½¦ç½‘æ–‡ä»¶")
        if yiche_file:
            try:
                df_yiche = pd.read_excel(yiche_file)
                st.success(f"âœ… æˆåŠŸè¯»å–æ˜“è½¦ç½‘æ–‡ä»¶ï¼Œå…± {len(df_yiche)} æ¡è®°å½•")
                st.dataframe(df_yiche.head(), use_container_width=True)
                
                # æ˜¾ç¤ºåˆ—å
                with st.expander("æŸ¥çœ‹æ–‡ä»¶åˆ—å"):
                    st.write("åˆ—ååˆ—è¡¨:", list(df_yiche.columns))
            except Exception as e:
                st.error(f"è¯»å–å¤±è´¥: {str(e)}")
        else:
            st.info("è¯·ä¸Šä¼ æ˜“è½¦ç½‘Excelæ–‡ä»¶")
    
    with col2:
        st.subheader("æ±½è½¦ä¹‹å®¶æ–‡ä»¶")
        if autohome_file:
            try:
                # å°è¯•å¤šç§ç¼–ç è¯»å–
                try:
                    df_autohome = pd.read_csv(autohome_file, encoding='utf-8')
                except:
                    try:
                        df_autohome = pd.read_csv(autohome_file, encoding='gbk')
                    except:
                        # å¦‚æœéƒ½ä¸è¡Œï¼Œå°è¯•è¯»å–åŸå§‹å­—èŠ‚å¹¶æ‰‹åŠ¨å¤„ç†
                        content = autohome_file.getvalue()
                        try:
                            content_str = content.decode('utf-8-sig')
                        except:
                            content_str = content.decode('utf-8', errors='ignore')
                        
                        # ä½¿ç”¨StringIOåŒ…è£…
                        df_autohome = pd.read_csv(io.StringIO(content_str))
                
                st.success(f"âœ… æˆåŠŸè¯»å–æ±½è½¦ä¹‹å®¶æ–‡ä»¶ï¼Œå…± {len(df_autohome)} æ¡è®°å½•")
                st.dataframe(df_autohome.head(), use_container_width=True)
                
                # æ˜¾ç¤ºåˆ—å
                with st.expander("æŸ¥çœ‹æ–‡ä»¶åˆ—å"):
                    st.write("åˆ—ååˆ—è¡¨:", list(df_autohome.columns))
                    
            except Exception as e:
                st.error(f"è¯»å–å¤±è´¥: {str(e)}")
                st.info("å»ºè®®å…ˆä½¿ç”¨å·¦ä¾§çš„'æ–‡ä»¶æ ¼å¼ä¿®å¤'åŠŸèƒ½å¤„ç†æ­¤æ–‡ä»¶")
        else:
            st.info("è¯·ä¸Šä¼ æ±½è½¦ä¹‹å®¶CSVæ–‡ä»¶")
    
    # æ˜¾ç¤ºä¿®å¤åçš„æ•°æ®ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
    if st.session_state.fixed_autohome_df is not None:
        st.subheader("ä¿®å¤åçš„æ±½è½¦ä¹‹å®¶æ•°æ®")
        st.dataframe(st.session_state.fixed_autohome_df.head(), use_container_width=True)
        st.success(f"ä¿®å¤åçš„æ•°æ®å…± {len(st.session_state.fixed_autohome_df)} æ¡è®°å½•")

with tab2:
    st.header("æ•°æ®å¤„ç†")
    
    # æ£€æŸ¥æ˜¯å¦æœ‰é€‰ä¸­çš„é”€å”®é¡¾é—®
    if not selected_consultants:
        st.warning("âš ï¸ è¯·å…ˆåœ¨ä¾§è¾¹æ é€‰æ‹©è‡³å°‘ä¸€ä¸ªé”€å”®é¡¾é—®")
    
    # æ£€æŸ¥æ˜¯å¦ä¸Šä¼ äº†æ–‡ä»¶
    files_available = (yiche_file is not None) or (autohome_file is not None) or (st.session_state.fixed_autohome_df is not None)
    
    if files_available and selected_consultants:
        if st.button("ğŸš€ å¼€å§‹åˆå¹¶å¤„ç†", type="primary"):
            with st.spinner("æ­£åœ¨å¤„ç†æ•°æ®..."):
                try:
                    # è¯»å–æ•°æ®
                    df_yiche = None
                    df_autohome = None
                    
                    if yiche_file:
                        df_yiche = pd.read_excel(yiche_file)
                    
                    # ä¼˜å…ˆä½¿ç”¨ä¿®å¤åçš„æ•°æ®
                    if st.session_state.fixed_autohome_df is not None:
                        df_autohome = st.session_state.fixed_autohome_df
                    elif autohome_file:
                        try:
                            df_autohome = pd.read_csv(autohome_file, encoding='utf-8')
                        except:
                            try:
                                df_autohome = pd.read_csv(autohome_file, encoding='gbk')
                            except:
                                content = autohome_file.getvalue()
                                content_str = content.decode('utf-8', errors='ignore')
                                df_autohome = pd.read_csv(io.StringIO(content_str))
                    
                    # å¤„ç†åˆå¹¶
                    if df_yiche is not None or df_autohome is not None:
                        df_result = process_merge(df_yiche, df_autohome, consultants, first_consultant)
                        
                        if df_result is not None:
                            # ä¿å­˜åˆ°session state
                            st.session_state.df_merged = df_result
                            
                            # æ˜¾ç¤ºå¤„ç†æ—¥å¿—
                            st.success(f"âœ… æ•°æ®å¤„ç†å®Œæˆï¼å…±åˆå¹¶ {len(df_result)} æ¡è®°å½•")
                            
                            # æ˜¾ç¤ºåˆ†é…ç»Ÿè®¡
                            st.subheader("é”€å”®é¡¾é—®åˆ†é…ç»Ÿè®¡")
                            allocation_counts = {}
                            for consultant in selected_consultants:
                                count = len(df_result[df_result['é”€å”®é¡¾é—®'] == consultant])
                                allocation_counts[consultant] = count
                                st.write(f"**{consultant}**: {count}æ¡")
                            
                            # æ£€æŸ¥åˆ†é…å‡åŒ€åº¦
                            counts = list(allocation_counts.values())
                            if counts:
                                max_count = max(counts)
                                min_count = min(counts)
                                if max_count - min_count > 1:
                                    st.warning(f"âš ï¸ åˆ†é…ä¸å‡åŒ€ï¼Œæœ€å¤§å·®å€¼ {max_count - min_count}")
                                else:
                                    st.success(f"âœ“ åˆ†é…å‡åŒ€ï¼Œæœ€å¤§å·®å€¼ {max_count - min_count}")
                    else:
                        st.error("æ²¡æœ‰å¯å¤„ç†çš„æ•°æ®æ–‡ä»¶")
                        
                except Exception as e:
                    st.error(f"å¤„ç†å¤±è´¥: {str(e)}")
                    st.info("é”™è¯¯è¯¦æƒ…:")
                    st.code(str(e))
    else:
        st.info("è¯·å…ˆä¸Šä¼ éœ€è¦å¤„ç†çš„æ–‡ä»¶ï¼Œå¹¶ç¡®ä¿å·²é€‰æ‹©è‡³å°‘ä¸€ä¸ªé”€å”®é¡¾é—®")

with tab3:
    st.header("ç»“æœåˆ†æä¸ä¸‹è½½")
    
    if st.session_state.df_merged is not None:
        df = st.session_state.df_merged
        
        # æ˜¾ç¤ºæ•°æ®é¢„è§ˆ
        st.subheader("ğŸ“‹ æ•°æ®é¢„è§ˆ")
        st.dataframe(df.head(20), use_container_width=True)
        
        # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
        col1, col2 = st.columns(2)
        
        with col1:
            st.subheader("ğŸ“Š åŸºæœ¬ç»Ÿè®¡")
            st.metric("æ€»è®°å½•æ•°", len(df))
            
            # è½¦ç³»ç»Ÿè®¡
            car_stats = df['æ„å‘è½¦ç³»'].value_counts()
            st.metric("è½¦å‹ç§ç±»", len(car_stats))
            
            # æ¥æºç»Ÿè®¡
            source_stats = df['çº¿ç´¢æ¥æº'].value_counts()
            st.metric("æ¥æºæ¸ é“", len(source_stats))
            
            # æ˜¾ç¤ºè½¦ç³»ç»Ÿè®¡è¯¦æƒ…
            with st.expander("æŸ¥çœ‹è½¦ç³»ç»Ÿè®¡è¯¦æƒ…"):
                for car, count in car_stats.items():
                    st.write(f"{car}: {count}æ¡")
            
        with col2:
            st.subheader("ğŸ” è½¦ç³»ç»Ÿè®¡å›¾è¡¨")
            if not df['æ„å‘è½¦ç³»'].empty:
                car_stats = df['æ„å‘è½¦ç³»'].value_counts()
                st.bar_chart(car_stats)
            
            # æ˜¾ç¤ºå‰5å¤§è½¦å‹
            st.subheader("ğŸ† å‰5å¤§è½¦å‹")
            top5 = car_stats.head(5)
            for i, (car, count) in enumerate(top5.items(), 1):
                percentage = (count / len(df)) * 100
                st.write(f"{i}. {car}: {count}æ¡ ({percentage:.1f}%)")
        
        # æ˜¾ç¤ºçº¿ç´¢æ¥æºç»Ÿè®¡
        st.subheader("ğŸ“ˆ çº¿ç´¢æ¥æºç»Ÿè®¡")
        col3, col4 = st.columns(2)
        
        with col3:
            if not df['çº¿ç´¢æ¥æº'].empty:
                source_stats = df['çº¿ç´¢æ¥æº'].value_counts()
                st.bar_chart(source_stats)
        
        with col4:
            with st.expander("æŸ¥çœ‹æ¥æºç»Ÿè®¡è¯¦æƒ…"):
                for source, count in source_stats.items():
                    st.write(f"{source}: {count}æ¡")
        
        # æä¾›ä¸‹è½½
        st.subheader("ğŸ’¾ ä¸‹è½½ç»“æœ")
        
        # è½¬æ¢ä¸ºExcel
        output = io.BytesIO()
        with pd.ExcelWriter(output, engine='openpyxl') as writer:
            df.to_excel(writer, index=False, sheet_name='åˆå¹¶ç»“æœ')
        output.seek(0)
        
        st.download_button(
            label="ğŸ“¥ ä¸‹è½½Excelæ–‡ä»¶",
            data=output,
            file_name=f"CRSçº¿ç´¢_{datetime.now().strftime('%Y%m%d_%H%M%S')}.xlsx",
            mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
            type="primary"
        )
        
        # æä¾›CSVæ ¼å¼ä¸‹è½½
        csv_output = df.to_csv(index=False, encoding='utf-8-sig')
        st.download_button(
            label="ğŸ“„ ä¸‹è½½CSVæ–‡ä»¶",
            data=csv_output,
            file_name=f"CRSçº¿ç´¢_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv",
            mime="text/csv"
        )
    else:
        st.info("è¯·å…ˆå¤„ç†æ•°æ®ä»¥æŸ¥çœ‹ç»“æœ")

# é¡µè„š
st.markdown("---")
st.caption("é”€å”®çº¿ç´¢åˆå¹¶å·¥å…· v1.0 | æŠ€æœ¯æ”¯æŒ")